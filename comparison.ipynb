{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.145640257Z",
     "start_time": "2023-07-11T21:23:33.898044565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "from duped_modules.dcn_duped import DCNDuped\n",
    "from duped_modules.dec_duped import DECDuped, IDECDuped\n",
    "from duped_modules.embeddings_autoencoder import EmbeddingsAutoencoder\n",
    "from duped_modules.gower_duped import gower_matrix as gower_matrix_duped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.334002658Z",
     "start_time": "2023-07-11T21:23:35.147321620Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "banking_df = pd.read_csv(\"datasets/banking_marketing.csv\", sep=\";\")\n",
    "banking_df.drop_duplicates(inplace=True)\n",
    "\n",
    "banking_y = banking_df[\"y\"]\n",
    "banking_y.hist()\n",
    "banking_y = LabelEncoder().fit_transform(banking_y)\n",
    "\n",
    "banking_cat_cols = [\"age\", \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"poutcome\"]\n",
    "banking_cont_cols = [\"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "\n",
    "banking_df.drop(columns=[\"y\", \"day\", \"month\"], axis=1, inplace=True)\n",
    "banking_df[banking_cat_cols] = banking_df[banking_cat_cols].apply(LabelEncoder().fit_transform)\n",
    "banking_df[banking_cont_cols] = StandardScaler().fit_transform(banking_df[banking_cont_cols])\n",
    "banking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.552199970Z",
     "start_time": "2023-07-11T21:23:35.335292907Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_df = pd.read_csv(\"datasets/census_income.csv\")\n",
    "census_df.drop_duplicates(inplace=True)\n",
    "\n",
    "census_df.loc[(census_df[\"class\"] == \" <=50K.\") | (census_df[\"class\"] == \" <=50K\"), \"class\"] = 0\n",
    "census_df.loc[(census_df[\"class\"] == \" >50K.\") | (census_df[\"class\"] == \" >50K\"), \"class\"] = 1\n",
    "census_df[\"class\"].hist()\n",
    "census_y = census_df[\"class\"].to_numpy()\n",
    "\n",
    "census_cat_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "census_cont_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "census_df.drop(columns=\"class\", inplace=True)\n",
    "census_df[census_cat_cols] = census_df[census_cat_cols].apply(LabelEncoder().fit_transform)\n",
    "census_df[census_cont_cols] = StandardScaler().fit_transform(census_df[census_cont_cols])\n",
    "census_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.657299480Z",
     "start_time": "2023-07-11T21:23:35.543026029Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"datasets/credit_approval.csv\")\n",
    "credit_df.replace(\"?\", pd.NA, inplace=True)\n",
    "credit_df.dropna(inplace=True)\n",
    "credit_df.drop_duplicates(inplace=True)\n",
    "\n",
    "credit_y = credit_df[\"A16\"]\n",
    "credit_y.hist()\n",
    "credit_y = LabelEncoder().fit_transform(credit_y)\n",
    "\n",
    "credit_cat_cols = [\"A1\", \"A4\", \"A5\", \"A6\", \"A7\", \"A9\", \"A10\", \"A12\", \"A13\"]\n",
    "credit_cont_cols = [\"A2\", \"A3\", \"A8\", \"A11\", \"A14\", \"A15\"]\n",
    "\n",
    "credit_df.drop(columns=\"A16\", inplace=True)\n",
    "credit_df[credit_cat_cols] = credit_df[credit_cat_cols].apply(LabelEncoder().fit_transform)\n",
    "credit_df[credit_cont_cols] = StandardScaler().fit_transform(credit_df[credit_cont_cols])\n",
    "credit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.820997656Z",
     "start_time": "2023-07-11T21:23:35.647120855Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv(\"datasets/heart_disease.csv\")\n",
    "heart_df.dropna(inplace=True)\n",
    "heart_df.drop_duplicates(inplace=True)\n",
    "\n",
    "heart_df[\"num\"].hist()\n",
    "heart_y = heart_df[\"num\"].to_numpy()\n",
    "\n",
    "heart_cat_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"slope\", \"thal\"]\n",
    "heart_cont_cols = [\"age\", \"trestbps\", \"chol\", \"thalch\", \"oldpeak\", \"ca\"]\n",
    "\n",
    "heart_df.drop(columns=[\"id\", \"dataset\", \"num\"], inplace=True)\n",
    "heart_df.dropna(inplace=True)\n",
    "\n",
    "heart_df[heart_cat_cols] = heart_df[heart_cat_cols].apply(LabelEncoder().fit_transform)\n",
    "heart_df[heart_cont_cols] = StandardScaler().fit_transform(heart_df[heart_cont_cols])\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.821247264Z",
     "start_time": "2023-07-11T21:23:35.806141127Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BankingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[banking_cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[banking_cont_cols].values, dtype=torch.float)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "\n",
    "banking_dataloader = DataLoader(BankingDataset(banking_df), batch_size=512, shuffle=True)\n",
    "\n",
    "banking_emb_sizes = [(banking_df[col].nunique(), min(50, banking_df[col].nunique()+1) // 2) for col in banking_df[banking_cat_cols]]\n",
    "banking_cat_dim = sum(d for _, d in banking_emb_sizes)\n",
    "banking_input_dim = banking_cat_dim + len(banking_cont_cols)\n",
    "\n",
    "print(f\"Cat dim: {banking_cat_dim}, Input dim: {banking_input_dim}\")\n",
    "print(f\"Embeddings: {banking_emb_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.821376008Z",
     "start_time": "2023-07-11T21:23:35.806240230Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CensusDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[census_cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[census_cont_cols].values, dtype=torch.float)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "\n",
    "census_dataloader = DataLoader(CensusDataset(census_df), batch_size=512, shuffle=True)\n",
    "\n",
    "census_emb_sizes = [(census_df[col].nunique(), min(50, census_df[col].nunique() + 1) // 2) for col in census_df[census_cat_cols]]\n",
    "census_cat_dim = sum(d for _, d in census_emb_sizes)\n",
    "census_input_dim = census_cat_dim + len(census_cont_cols)\n",
    "\n",
    "print(f\"Cat dim: {census_cat_dim}, Input dim: {census_input_dim}\")\n",
    "print(f\"Embeddings: {census_emb_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.821545394Z",
     "start_time": "2023-07-11T21:23:35.806271551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CreditDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[credit_cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[credit_cont_cols].values, dtype=torch.float)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "\n",
    "credit_dataloader = DataLoader(CreditDataset(credit_df), batch_size=32, shuffle=True)\n",
    "\n",
    "credit_emb_sizes = [(credit_df[col].nunique(), min(50, credit_df[col].nunique() + 1) // 2) for col in credit_df[credit_cat_cols]]\n",
    "credit_cat_dim = sum(d for _, d in credit_emb_sizes)\n",
    "credit_input_dim = credit_cat_dim + len(credit_cont_cols)\n",
    "\n",
    "print(f\"Cat dim: {credit_cat_dim}, Input dim: {credit_input_dim}\")\n",
    "print(f\"Embeddings: {credit_emb_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.822037928Z",
     "start_time": "2023-07-11T21:23:35.806299366Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[heart_cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[heart_cont_cols].values, dtype=torch.float)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "\n",
    "heart_dataloader = DataLoader(HeartDataset(heart_df), batch_size=64, shuffle=True)\n",
    "\n",
    "heart_emb_sizes = [(heart_df[col].nunique(), min(50, heart_df[col].nunique() + 1) // 2) for col in heart_df[heart_cat_cols]]\n",
    "heart_cat_dim = sum(d for n, d in heart_emb_sizes)\n",
    "heart_input_dim = heart_cat_dim + len(heart_cont_cols)\n",
    "\n",
    "print(f\"Cat dim: {heart_cat_dim}, Input dim: {heart_input_dim}\")\n",
    "print(f\"Embeddings: {heart_emb_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.822199214Z",
     "start_time": "2023-07-11T21:23:35.815298578Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# banking_k_prototypes = KPrototypes(n_clusters=2, random_state=0, verbose=1, n_jobs=-1)\n",
    "# banking_k_prototypes.fit(banking_df.values, categorical=[banking_df.columns.get_loc(col) for col in banking_cat_cols])\n",
    "# banking_k_prototypes_nmi = normalized_mutual_info_score(banking_y, banking_k_prototypes.labels_)\n",
    "# banking_k_prototypes_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.867209747Z",
     "start_time": "2023-07-11T21:23:35.823136276Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# census_k_prototypes = KPrototypes(n_clusters=2, random_state=0, verbose=1, n_jobs=-1)\n",
    "# census_k_prototypes.fit(census_df.values, categorical=[census_df.columns.get_loc(col) for col in census_cat_cols])\n",
    "# census_k_prototypes_nmi = normalized_mutual_info_score(census_y, census_k_prototypes.labels_)\n",
    "# census_k_prototypes_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.867430254Z",
     "start_time": "2023-07-11T21:23:35.866071822Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# credit_k_prototypes = KPrototypes(n_clusters=2, random_state=0, verbose=1, n_jobs=-1)\n",
    "# credit_k_prototypes.fit(credit_df.values, categorical=[credit_df.columns.get_loc(col) for col in credit_cat_cols])\n",
    "# credit_k_prototypes_nmi = normalized_mutual_info_score(credit_y, credit_k_prototypes.labels_)\n",
    "# credit_k_prototypes_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:23:35.867515448Z",
     "start_time": "2023-07-11T21:23:35.866159424Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# heart_k_prototypes = KPrototypes(n_clusters=5, random_state=0, verbose=1, n_jobs=-1)\n",
    "# heart_k_prototypes.fit(heart_df.values, categorical=[heart_df.columns.get_loc(col) for col in heart_cat_cols])\n",
    "# heart_k_prototypes_nmi = normalized_mutual_info_score(heart_y, heart_k_prototypes.labels_)\n",
    "# heart_k_prototypes_nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:24:43.454419225Z",
     "start_time": "2023-07-11T21:23:35.866185722Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "banking_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(banking_input_dim, 32),\n",
    "    torch.nn.BatchNorm1d(32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 16),\n",
    "    torch.nn.BatchNorm1d(16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 12),\n",
    "    torch.nn.BatchNorm1d(12),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "banking_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(12, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, banking_cat_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "banking_ae = EmbeddingsAutoencoder(banking_encoder, banking_decoder, banking_input_dim, banking_cat_dim, banking_emb_sizes, attention=False)\n",
    "banking_ae.fit(n_epochs=100, lr=0.001, dataloader=banking_dataloader)\n",
    "\n",
    "banking_attention_ae = EmbeddingsAutoencoder(banking_encoder, banking_decoder, banking_input_dim, banking_cat_dim, banking_emb_sizes, attention=True)\n",
    "banking_attention_ae.fit(n_epochs=100, lr=0.001, dataloader=banking_dataloader)\n",
    "\n",
    "cat = torch.tensor(banking_df[banking_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(banking_df[banking_cont_cols].values, dtype=torch.float)\n",
    "cat_features = banking_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, banking_df[banking_cont_cols].values), 1)\n",
    "banking_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "banking_ae_nmi = normalized_mutual_info_score(banking_y, banking_ae_kmeans.labels_)\n",
    "print(banking_ae_nmi)\n",
    "\n",
    "cat = torch.tensor(banking_df[banking_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(banking_df[banking_cont_cols].values, dtype=torch.float)\n",
    "cat_features = banking_attention_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, banking_df[banking_cont_cols].values), 1)\n",
    "banking_attention_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "banking_attention_ae_nmi = normalized_mutual_info_score(banking_y, banking_attention_ae_kmeans.labels_)\n",
    "print(banking_attention_ae_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:25:58.113274846Z",
     "start_time": "2023-07-11T21:24:43.455837546Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(census_input_dim, 32),\n",
    "    torch.nn.BatchNorm1d(32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 16),\n",
    "    torch.nn.BatchNorm1d(16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 12),\n",
    "    torch.nn.BatchNorm1d(12),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "census_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(12, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, census_cat_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "census_ae = EmbeddingsAutoencoder(census_encoder, census_decoder, census_input_dim, census_cat_dim, census_emb_sizes, attention=False)\n",
    "census_ae.fit(n_epochs=100, lr=0.001, dataloader=census_dataloader)\n",
    "\n",
    "census_attention_ae = EmbeddingsAutoencoder(census_encoder, census_decoder, census_input_dim, census_cat_dim, census_emb_sizes, attention=True)\n",
    "census_attention_ae.fit(n_epochs=100, lr=0.001, dataloader=census_dataloader)\n",
    "\n",
    "cat = torch.tensor(census_df[census_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(census_df[census_cont_cols].values, dtype=torch.float)\n",
    "cat_features = census_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, census_df[census_cont_cols].values), 1)\n",
    "census_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "census_ae_nmi = normalized_mutual_info_score(census_y, census_ae_kmeans.labels_)\n",
    "print(census_ae_nmi)\n",
    "\n",
    "cat = torch.tensor(census_df[census_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(census_df[census_cont_cols].values, dtype=torch.float)\n",
    "cat_features = census_attention_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, census_df[census_cont_cols].values), 1)\n",
    "census_attention_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "census_attention_ae_nmi = normalized_mutual_info_score(census_y, census_attention_ae_kmeans.labels_)\n",
    "print(census_attention_ae_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:26:05.584030275Z",
     "start_time": "2023-07-11T21:25:58.117030312Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(credit_input_dim, 16),\n",
    "    torch.nn.BatchNorm1d(16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 8),\n",
    "    torch.nn.BatchNorm1d(8),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(8, 6),\n",
    "    torch.nn.BatchNorm1d(6),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "credit_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(6, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, credit_cat_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "credit_ae = EmbeddingsAutoencoder(credit_encoder, credit_decoder, credit_input_dim, credit_cat_dim, credit_emb_sizes, attention=False)\n",
    "credit_ae.fit(n_epochs=100, lr=0.001, dataloader=credit_dataloader)\n",
    "\n",
    "credit_attention_ae = EmbeddingsAutoencoder(credit_encoder, credit_decoder, credit_input_dim, credit_cat_dim, credit_emb_sizes, attention=True)\n",
    "credit_attention_ae.fit(n_epochs=100, lr=0.001, dataloader=credit_dataloader)\n",
    "\n",
    "cat = torch.tensor(credit_df[credit_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(credit_df[credit_cont_cols].values, dtype=torch.float)\n",
    "cat_features = credit_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, credit_df[credit_cont_cols].values), 1)\n",
    "credit_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "credit_ae_nmi = normalized_mutual_info_score(credit_y, credit_ae_kmeans.labels_)\n",
    "print(credit_ae_nmi)\n",
    "\n",
    "cat = torch.tensor(credit_df[credit_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(credit_df[credit_cont_cols].values, dtype=torch.float)\n",
    "cat_features = credit_attention_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, credit_df[credit_cont_cols].values), 1)\n",
    "credit_attention_ae_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "credit_attention_ae_nmi = normalized_mutual_info_score(credit_y, credit_attention_ae_kmeans.labels_)\n",
    "print(credit_attention_ae_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:26:07.550889108Z",
     "start_time": "2023-07-11T21:26:05.583593951Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_encoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(heart_input_dim, 8),\n",
    "    torch.nn.BatchNorm1d(8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8, 5),\n",
    "    torch.nn.BatchNorm1d(5),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(5, 4),\n",
    "    torch.nn.BatchNorm1d(4),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "heart_decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(5, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8, heart_cat_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "heart_ae = EmbeddingsAutoencoder(heart_encoder, heart_decoder, heart_input_dim, heart_cat_dim, heart_emb_sizes, attention=False)\n",
    "heart_ae.fit(n_epochs=100, lr=0.001, dataloader=heart_dataloader)\n",
    "\n",
    "heart_attention_ae = EmbeddingsAutoencoder(heart_encoder, heart_decoder, heart_input_dim, heart_cat_dim, heart_emb_sizes, attention=True)\n",
    "heart_attention_ae.fit(n_epochs=100, lr=0.001, dataloader=heart_dataloader)\n",
    "\n",
    "cat = torch.tensor(heart_df[heart_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(heart_df[heart_cont_cols].values, dtype=torch.float)\n",
    "cat_features = heart_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, heart_df[heart_cont_cols].values), 1)\n",
    "heart_ae_kmeans = KMeans(n_clusters=5, n_init=\"auto\", random_state=0).fit(features)\n",
    "heart_ae_nmi = normalized_mutual_info_score(heart_y, heart_ae_kmeans.labels_)\n",
    "print(heart_ae_nmi)\n",
    "\n",
    "cat = torch.tensor(heart_df[heart_cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(heart_df[heart_cont_cols].values, dtype=torch.float)\n",
    "cat_features = heart_attention_ae.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, heart_df[heart_cont_cols].values), 1)\n",
    "heart_attention_ae_kmeans = KMeans(n_clusters=5, n_init=\"auto\", random_state=0).fit(features)\n",
    "heart_attention_ae_nmi = normalized_mutual_info_score(heart_y, heart_attention_ae_kmeans.labels_)\n",
    "print(heart_attention_ae_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:30:53.578345013Z",
     "start_time": "2023-07-11T21:26:07.552317522Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "banking_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_ae, random_state=np.random.RandomState(0))\n",
    "banking_dcn.fit(banking_dataloader)\n",
    "banking_dcn_nmi = normalized_mutual_info_score(banking_y, banking_dcn.labels_)\n",
    "print(banking_dcn_nmi)\n",
    "\n",
    "banking_attention_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_attention_ae, random_state=np.random.RandomState(0))\n",
    "banking_attention_dcn.fit(banking_dataloader)\n",
    "banking_attention_dcn_nmi = normalized_mutual_info_score(banking_y, banking_attention_dcn.labels_)\n",
    "print(banking_attention_dcn_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:36:00.506206663Z",
     "start_time": "2023-07-11T21:30:53.580067036Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_ae, random_state=np.random.RandomState(0))\n",
    "census_dcn.fit(census_dataloader)\n",
    "census_dcn_nmi = normalized_mutual_info_score(census_y, census_dcn.labels_)\n",
    "print(census_dcn_nmi)\n",
    "\n",
    "census_attention_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_attention_ae, random_state=np.random.RandomState(0))\n",
    "census_attention_dcn.fit(census_dataloader)\n",
    "census_attention_dcn_nmi = normalized_mutual_info_score(census_y, census_attention_dcn.labels_)\n",
    "print(census_attention_dcn_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:36:15.552398554Z",
     "start_time": "2023-07-11T21:36:00.507971529Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_ae, random_state=np.random.RandomState(0))\n",
    "credit_dcn.fit(credit_dataloader)\n",
    "credit_dcn_nmi = normalized_mutual_info_score(credit_y, credit_dcn.labels_)\n",
    "print(credit_dcn_nmi)\n",
    "\n",
    "credit_attention_dcn = DCNDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_attention_ae, random_state=np.random.RandomState(0))\n",
    "credit_attention_dcn.fit(credit_dataloader)\n",
    "credit_attention_dcn_nmi = normalized_mutual_info_score(credit_y, credit_attention_dcn.labels_)\n",
    "print(credit_attention_dcn_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:36:20.149227939Z",
     "start_time": "2023-07-11T21:36:15.553603785Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_dcn = DCNDuped(n_clusters=5, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_ae, random_state=np.random.RandomState(0))\n",
    "heart_dcn.fit(heart_dataloader)\n",
    "heart_dcn_nmi = normalized_mutual_info_score(heart_y, heart_dcn.labels_)\n",
    "print(heart_dcn_nmi)\n",
    "\n",
    "heart_attention_dcn = DCNDuped(n_clusters=5, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_attention_ae, random_state=np.random.RandomState(0))\n",
    "heart_attention_dcn.fit(heart_dataloader)\n",
    "heart_attention_dcn_nmi = normalized_mutual_info_score(heart_y, heart_attention_dcn.labels_)\n",
    "print(heart_attention_dcn_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:37:25.895344326Z",
     "start_time": "2023-07-11T21:36:20.150531444Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "banking_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_ae, random_state=np.random.RandomState(0))\n",
    "banking_dec.fit(banking_dataloader)\n",
    "banking_dec_nmi = normalized_mutual_info_score(banking_y, banking_dec.labels_)\n",
    "print(banking_dec_nmi)\n",
    "\n",
    "banking_attention_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_attention_ae, random_state=np.random.RandomState(0))\n",
    "banking_attention_dec.fit(banking_dataloader)\n",
    "banking_attention_dec_nmi = normalized_mutual_info_score(banking_y, banking_attention_dec.labels_)\n",
    "print(banking_attention_dec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:38:36.391235503Z",
     "start_time": "2023-07-11T21:37:25.897327453Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_ae, random_state=np.random.RandomState(0))\n",
    "census_dec.fit(census_dataloader)\n",
    "census_dec_nmi = normalized_mutual_info_score(census_y, census_dec.labels_)\n",
    "print(census_dec_nmi)\n",
    "\n",
    "census_attention_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_attention_ae, random_state=np.random.RandomState(0))\n",
    "census_attention_dec.fit(census_dataloader)\n",
    "census_attention_dec_nmi = normalized_mutual_info_score(census_y, census_attention_dec.labels_)\n",
    "print(census_attention_dec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:38:43.848270518Z",
     "start_time": "2023-07-11T21:38:36.392788187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_ae, random_state=np.random.RandomState(0))\n",
    "credit_dec.fit(credit_dataloader)\n",
    "credit_dec_nmi = normalized_mutual_info_score(credit_y, credit_dec.labels_)\n",
    "print(credit_dec_nmi)\n",
    "\n",
    "credit_attention_dec = DECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_attention_ae, random_state=np.random.RandomState(0))\n",
    "credit_attention_dec.fit(credit_dataloader)\n",
    "credit_attention_dec_nmi = normalized_mutual_info_score(credit_y, credit_attention_dec.labels_)\n",
    "print(credit_attention_dec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:38:45.833332496Z",
     "start_time": "2023-07-11T21:38:43.849617385Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_dec = DECDuped(n_clusters=5, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_ae, random_state=np.random.RandomState(0))\n",
    "heart_dec.fit(heart_dataloader)\n",
    "heart_dec_nmi = normalized_mutual_info_score(heart_y, heart_dec.labels_)\n",
    "print(heart_dec_nmi)\n",
    "\n",
    "heart_attention_dec = DECDuped(n_clusters=5, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_attention_ae, random_state=np.random.RandomState(0))\n",
    "heart_attention_dec.fit(heart_dataloader)\n",
    "heart_attention_dec_nmi = normalized_mutual_info_score(heart_y, heart_attention_dec.labels_)\n",
    "print(heart_attention_dec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:39:55.648824929Z",
     "start_time": "2023-07-11T21:38:45.835281463Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "banking_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_ae, random_state=np.random.RandomState(0))\n",
    "banking_idec.fit(banking_dataloader)\n",
    "banking_idec_nmi = normalized_mutual_info_score(banking_y, banking_idec.labels_)\n",
    "print(banking_idec_nmi)\n",
    "\n",
    "banking_attention_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=banking_attention_ae, random_state=np.random.RandomState(0))\n",
    "banking_attention_idec.fit(banking_dataloader)\n",
    "banking_attention_idec_nmi = normalized_mutual_info_score(banking_y, banking_attention_idec.labels_)\n",
    "print(banking_attention_idec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:41:11.647282671Z",
     "start_time": "2023-07-11T21:39:55.653015687Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "census_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_ae, random_state=np.random.RandomState(0))\n",
    "census_idec.fit(census_dataloader)\n",
    "census_idec_nmi = normalized_mutual_info_score(census_y, census_idec.labels_)\n",
    "print(census_idec_nmi)\n",
    "\n",
    "census_attention_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=census_attention_ae, random_state=np.random.RandomState(0))\n",
    "census_attention_idec.fit(census_dataloader)\n",
    "census_attention_idec_nmi = normalized_mutual_info_score(census_y, census_attention_idec.labels_)\n",
    "print(census_attention_idec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:41:20.575957527Z",
     "start_time": "2023-07-11T21:41:11.648670821Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_ae, random_state=np.random.RandomState(0))\n",
    "credit_idec.fit(credit_dataloader)\n",
    "credit_idec_nmi = normalized_mutual_info_score(credit_y, credit_idec.labels_)\n",
    "print(credit_idec_nmi)\n",
    "\n",
    "credit_attention_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=credit_attention_ae, random_state=np.random.RandomState(0))\n",
    "credit_attention_idec.fit(credit_dataloader)\n",
    "credit_attention_idec_nmi = normalized_mutual_info_score(credit_y, credit_attention_idec.labels_)\n",
    "print(credit_attention_idec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:41:22.672649508Z",
     "start_time": "2023-07-11T21:41:20.577449274Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_ae, random_state=np.random.RandomState(0))\n",
    "heart_idec.fit(heart_dataloader)\n",
    "heart_idec_nmi = normalized_mutual_info_score(heart_y, heart_idec.labels_)\n",
    "print(heart_idec_nmi)\n",
    "\n",
    "heart_attention_idec = IDECDuped(n_clusters=2, pretrain_epochs=100, clustering_epochs=100, autoencoder=heart_attention_ae, random_state=np.random.RandomState(0))\n",
    "heart_attention_idec.fit(heart_dataloader)\n",
    "heart_attention_idec_nmi = normalized_mutual_info_score(heart_y, heart_attention_idec.labels_)\n",
    "print(heart_attention_idec_nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T21:41:41.382041675Z",
     "start_time": "2023-07-11T21:41:41.330067816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame([\n",
    "    [0.017825373955683156, 0.0224625641297901, 0.10895828404430437, 0.17059947018933327],\n",
    "    [pd.NA, 0.000023, pd.NA, pd.NA],\n",
    "    [banking_ae_nmi, census_ae_nmi, credit_ae_nmi, heart_ae_nmi],\n",
    "    [banking_attention_ae_nmi, census_attention_ae_nmi, credit_attention_ae_nmi, heart_attention_ae_nmi],\n",
    "    [banking_dcn_nmi, census_dcn_nmi, credit_dcn_nmi, heart_dcn_nmi],\n",
    "    [banking_attention_dcn_nmi, census_attention_dcn_nmi, credit_attention_dcn_nmi, heart_attention_dcn_nmi],\n",
    "    [banking_dec_nmi, census_dec_nmi, credit_dec_nmi, heart_dec_nmi],\n",
    "    [banking_attention_dec_nmi, census_attention_dec_nmi, credit_attention_dec_nmi, heart_attention_dec_nmi],\n",
    "    [banking_idec_nmi, census_idec_nmi, credit_idec_nmi, heart_idec_nmi],\n",
    "    [banking_attention_idec_nmi, census_attention_idec_nmi, credit_attention_idec_nmi, heart_attention_idec_nmi],],\n",
    "    index=[\n",
    "        \"K-Prototypes\",\n",
    "        \"Gower + Agglomerative\",\n",
    "        \"AE + k-means\",\n",
    "        \"Attention AE + k-means\",\n",
    "        \"DCN\",\n",
    "        \"Attention DCN\",\n",
    "        \"DEC\",\n",
    "        \"Attention DEC\",\n",
    "        \"IDEC\",\n",
    "        \"Attention IDEC\",],\n",
    "    columns=[\"Banking Marketing\", \"Adult/Census Income\", \"Credit Approval\", \"Heart Disease\"])\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
