{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from duped_modules.dcn_duped import DCNDuped\n",
    "from duped_modules.dec_duped import DECDuped, IDECDuped\n",
    "from basic_autoencoder import BasicAutoencoder\n",
    "from load_datasets import load_all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_accuracy(y_true, y_pred):\n",
    "    # We need to map the labels to our cluster labels\n",
    "    # This is a linear assignment problem on a bipartite graph\n",
    "    k = max(len(np.unique(y_true)), len(np.unique(y_pred)))\n",
    "    cost_matrix = np.zeros((k, k))\n",
    "    for i in range(y_true.shape[0]):\n",
    "        cost_matrix[y_true[i], y_pred[i]] += 1\n",
    "    inverted_cost_matrix = cost_matrix.max() - cost_matrix\n",
    "    row_ind, col_ind = linear_sum_assignment(inverted_cost_matrix)\n",
    "    return cost_matrix[row_ind, col_ind].sum() / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_all_datasets(max_rows=5000)\n",
    "\n",
    "accuracies = {d.name: {} for d in datasets}\n",
    "nmis = {d.name: {} for d in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abalone: Input dim: 9; Cat dim: 2; Cont dim: 7\n",
      "Auction Verification: Input dim: 13; Cat dim: 12; Cont dim: 1\n",
      "Bank Marketing: Input dim: 22; Cat dim: 17; Cont dim: 5\n",
      "Breast Cancer: Input dim: 45; Cat dim: 45; Cont dim: 0\n",
      "Census Income: Input dim: 58; Cat dim: 52; Cont dim: 6\n",
      "Credit Approval: Input dim: 28; Cat dim: 22; Cont dim: 6\n",
      "Heart Disease: Input dim: 17; Cat dim: 11; Cont dim: 6\n",
      "Soybean Disease: Input dim: 55; Cat dim: 55; Cont dim: 0\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    print(f\"{d.name}: Input dim: {d.input_dim}; Cat dim: {d.cat_dim}; Cont dim: {d.cont_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Batch Reconstruction loss: 0.124038\n",
      "Epoch 50/100 - Batch Reconstruction loss: 0.106587\n",
      "Epoch 75/100 - Batch Reconstruction loss: 0.120869\n",
      "Epoch 100/100 - Batch Reconstruction loss: 0.171200\n",
      "0.1585459416556321\n"
     ]
    }
   ],
   "source": [
    "abalone_encoder = nn.Sequential(\n",
    "    nn.Linear(datasets.abalone.input_dim, 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6, 4),\n",
    "    nn.LeakyReLU(),\n",
    ")\n",
    "abalone_decoder = nn.Sequential(\n",
    "    nn.Linear(4, 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6, datasets.abalone.input_dim),\n",
    "    nn.LeakyReLU(),\n",
    ")\n",
    "\n",
    "ae = BasicAutoencoder(deepcopy(abalone_encoder), deepcopy(abalone_decoder), datasets.abalone.embedding_sizes)\n",
    "ae.fit(datasets.abalone.dataloader, n_epochs=100, lr=0.001)\n",
    "\n",
    "cat = torch.tensor(datasets.abalone.df[datasets.abalone.cat_cols].values, dtype=torch.int).detach()\n",
    "cont = torch.tensor(datasets.abalone.df[datasets.abalone.cont_cols].values, dtype=torch.float).detach()\n",
    "features = ae.encode(cat, cont).detach().numpy()\n",
    "kmeans = KMeans(n_clusters=datasets.abalone.n_targets, init=\"random\", n_init=10, max_iter=300, random_state=0, algorithm=\"lloyd\").fit(features)\n",
    "nmi = normalized_mutual_info_score(datasets.abalone.y, kmeans.labels_)\n",
    "print(nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
