{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.215189Z",
     "end_time": "2023-07-06T00:22:12.229027Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from gower_duped import gower_matrix as gower_matrix_duped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.218195Z",
     "end_time": "2023-07-06T00:22:12.230032Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_accuracy(y_pred, y_true):\n",
    "    # We need to map the labels to our cluster labels\n",
    "    # This is a linear assignment problem on a bipartite graph\n",
    "    k = max(len(np.unique(y_pred)), len(np.unique(y_pred)))\n",
    "    cost_matrix = np.zeros((k, k))\n",
    "    for i in range(y_pred.size):\n",
    "        cost_matrix[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix.max() - cost_matrix)\n",
    "    return cost_matrix[row_ind, col_ind].sum() / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.230032Z",
     "end_time": "2023-07-06T00:22:12.308803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       age          workclass  fnlwgt   education  education-num   \n0       39          State-gov   77516   Bachelors             13  \\\n1       50   Self-emp-not-inc   83311   Bachelors             13   \n2       38            Private  215646     HS-grad              9   \n3       53            Private  234721        11th              7   \n4       28            Private  338409   Bachelors             13   \n...    ...                ...     ...         ...            ...   \n48837   39            Private  215419   Bachelors             13   \n48838   64                  ?  321403     HS-grad              9   \n48839   38            Private  374983   Bachelors             13   \n48840   44            Private   83891   Bachelors             13   \n48841   35       Self-emp-inc  182148   Bachelors             13   \n\n            marital-status          occupation     relationship   \n0            Never-married        Adm-clerical    Not-in-family  \\\n1       Married-civ-spouse     Exec-managerial          Husband   \n2                 Divorced   Handlers-cleaners    Not-in-family   \n3       Married-civ-spouse   Handlers-cleaners          Husband   \n4       Married-civ-spouse      Prof-specialty             Wife   \n...                    ...                 ...              ...   \n48837             Divorced      Prof-specialty    Not-in-family   \n48838              Widowed                   ?   Other-relative   \n48839   Married-civ-spouse      Prof-specialty          Husband   \n48840             Divorced        Adm-clerical        Own-child   \n48841   Married-civ-spouse     Exec-managerial          Husband   \n\n                      race      sex  capital-gain  capital-loss   \n0                    White     Male          2174             0  \\\n1                    White     Male             0             0   \n2                    White     Male             0             0   \n3                    Black     Male             0             0   \n4                    Black   Female             0             0   \n...                    ...      ...           ...           ...   \n48837                White   Female             0             0   \n48838                Black     Male             0             0   \n48839                White     Male             0             0   \n48840   Asian-Pac-Islander     Male          5455             0   \n48841                White     Male             0             0   \n\n       hours-per-week  native-country    class  \n0                  40   United-States    <=50K  \n1                  13   United-States    <=50K  \n2                  40   United-States    <=50K  \n3                  40   United-States    <=50K  \n4                  40            Cuba    <=50K  \n...               ...             ...      ...  \n48837              36   United-States   <=50K.  \n48838              40   United-States   <=50K.  \n48839              50   United-States   <=50K.  \n48840              40   United-States   <=50K.  \n48841              60   United-States    >50K.  \n\n[48842 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>39</td>\n      <td>Private</td>\n      <td>215419</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Prof-specialty</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>64</td>\n      <td>?</td>\n      <td>321403</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Widowed</td>\n      <td>?</td>\n      <td>Other-relative</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>374983</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>83891</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>Asian-Pac-Islander</td>\n      <td>Male</td>\n      <td>5455</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>35</td>\n      <td>Self-emp-inc</td>\n      <td>182148</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>United-States</td>\n      <td>&gt;50K.</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df = pd.read_csv(\"datasets/census_income.csv\")\n",
    "og_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7607182343065395"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df.loc[(og_df[\"class\"] == \" <=50K.\") | (og_df[\"class\"] == \" <=50K\"), \"class\"] = 0\n",
    "og_df.loc[(og_df[\"class\"] == \" >50K.\") | (og_df[\"class\"] == \" >50K\"), \"class\"] = 1\n",
    "# Probability of most common class\n",
    "og_df[\"class\"].value_counts().max()/og_df[\"class\"].count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.308803Z",
     "end_time": "2023-07-06T00:22:12.327612Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.329051Z",
     "end_time": "2023-07-06T00:22:12.329557Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "cont_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.331920Z",
     "end_time": "2023-07-06T00:22:12.427759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age  workclass    fnlwgt  education  education-num   \n0      0.301370          7  0.044131          9       0.800000  \\\n1      0.452055          6  0.048052          9       0.800000   \n2      0.287671          4  0.137581         11       0.533333   \n3      0.493151          4  0.150486          1       0.400000   \n4      0.150685          4  0.220635          9       0.800000   \n...         ...        ...       ...        ...            ...   \n48837  0.301370          4  0.137428          9       0.800000   \n48838  0.643836          0  0.209130         11       0.533333   \n48839  0.287671          4  0.245379          9       0.800000   \n48840  0.369863          4  0.048444          9       0.800000   \n48841  0.246575          5  0.114919          9       0.800000   \n\n       marital-status  occupation  relationship  race  sex  capital-gain   \n0                   4           1             1     4    1      0.021740  \\\n1                   2           4             0     4    1      0.000000   \n2                   0           6             1     4    1      0.000000   \n3                   2           6             0     2    1      0.000000   \n4                   2          10             5     2    0      0.000000   \n...               ...         ...           ...   ...  ...           ...   \n48837               0          10             1     4    0      0.000000   \n48838               6           0             2     2    1      0.000000   \n48839               2          10             0     4    1      0.000000   \n48840               0           1             3     1    1      0.054551   \n48841               2           4             0     4    1      0.000000   \n\n       capital-loss  hours-per-week  native-country  \n0               0.0        0.397959              39  \n1               0.0        0.122449              39  \n2               0.0        0.397959              39  \n3               0.0        0.397959              39  \n4               0.0        0.397959               5  \n...             ...             ...             ...  \n48837           0.0        0.357143              39  \n48838           0.0        0.397959              39  \n48839           0.0        0.500000              39  \n48840           0.0        0.397959              39  \n48841           0.0        0.602041              39  \n\n[48842 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>7</td>\n      <td>0.044131</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>6</td>\n      <td>0.048052</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.137581</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>4</td>\n      <td>0.150486</td>\n      <td>1</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>4</td>\n      <td>0.220635</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>4</td>\n      <td>0.137428</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0</td>\n      <td>0.209130</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.245379</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>4</td>\n      <td>0.048444</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>5</td>\n      <td>0.114919</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = og_df.copy()\n",
    "df.drop(columns=\"class\", inplace=True)\n",
    "df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)\n",
    "df[cont_cols] = MinMaxScaler().fit_transform(df[cont_cols])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.395947Z",
     "end_time": "2023-07-06T00:22:12.615442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age    fnlwgt  education-num  capital-gain  capital-loss   \n0      0.301370  0.044131       0.800000      0.021740           0.0  \\\n1      0.452055  0.048052       0.800000      0.000000           0.0   \n2      0.287671  0.137581       0.533333      0.000000           0.0   \n3      0.493151  0.150486       0.400000      0.000000           0.0   \n4      0.150685  0.220635       0.800000      0.000000           0.0   \n...         ...       ...            ...           ...           ...   \n48837  0.301370  0.137428       0.800000      0.000000           0.0   \n48838  0.643836  0.209130       0.533333      0.000000           0.0   \n48839  0.287671  0.245379       0.800000      0.000000           0.0   \n48840  0.369863  0.048444       0.800000      0.054551           0.0   \n48841  0.246575  0.114919       0.800000      0.000000           0.0   \n\n       hours-per-week  workclass_ ?  workclass_ Federal-gov   \n0            0.397959           0.0                     0.0  \\\n1            0.122449           0.0                     0.0   \n2            0.397959           0.0                     0.0   \n3            0.397959           0.0                     0.0   \n4            0.397959           0.0                     0.0   \n...               ...           ...                     ...   \n48837        0.357143           0.0                     0.0   \n48838        0.397959           1.0                     0.0   \n48839        0.500000           0.0                     0.0   \n48840        0.397959           0.0                     0.0   \n48841        0.602041           0.0                     0.0   \n\n       workclass_ Local-gov  workclass_ Never-worked  ...   \n0                       0.0                      0.0  ...  \\\n1                       0.0                      0.0  ...   \n2                       0.0                      0.0  ...   \n3                       0.0                      0.0  ...   \n4                       0.0                      0.0  ...   \n...                     ...                      ...  ...   \n48837                   0.0                      0.0  ...   \n48838                   0.0                      0.0  ...   \n48839                   0.0                      0.0  ...   \n48840                   0.0                      0.0  ...   \n48841                   0.0                      0.0  ...   \n\n       native-country_ Portugal  native-country_ Puerto-Rico   \n0                           0.0                          0.0  \\\n1                           0.0                          0.0   \n2                           0.0                          0.0   \n3                           0.0                          0.0   \n4                           0.0                          0.0   \n...                         ...                          ...   \n48837                       0.0                          0.0   \n48838                       0.0                          0.0   \n48839                       0.0                          0.0   \n48840                       0.0                          0.0   \n48841                       0.0                          0.0   \n\n       native-country_ Scotland  native-country_ South   \n0                           0.0                    0.0  \\\n1                           0.0                    0.0   \n2                           0.0                    0.0   \n3                           0.0                    0.0   \n4                           0.0                    0.0   \n...                         ...                    ...   \n48837                       0.0                    0.0   \n48838                       0.0                    0.0   \n48839                       0.0                    0.0   \n48840                       0.0                    0.0   \n48841                       0.0                    0.0   \n\n       native-country_ Taiwan  native-country_ Thailand   \n0                         0.0                       0.0  \\\n1                         0.0                       0.0   \n2                         0.0                       0.0   \n3                         0.0                       0.0   \n4                         0.0                       0.0   \n...                       ...                       ...   \n48837                     0.0                       0.0   \n48838                     0.0                       0.0   \n48839                     0.0                       0.0   \n48840                     0.0                       0.0   \n48841                     0.0                       0.0   \n\n       native-country_ Trinadad&Tobago  native-country_ United-States   \n0                                  0.0                            1.0  \\\n1                                  0.0                            1.0   \n2                                  0.0                            1.0   \n3                                  0.0                            1.0   \n4                                  0.0                            0.0   \n...                                ...                            ...   \n48837                              0.0                            1.0   \n48838                              0.0                            1.0   \n48839                              0.0                            1.0   \n48840                              0.0                            1.0   \n48841                              0.0                            1.0   \n\n       native-country_ Vietnam  native-country_ Yugoslavia  \n0                          0.0                         0.0  \n1                          0.0                         0.0  \n2                          0.0                         0.0  \n3                          0.0                         0.0  \n4                          0.0                         0.0  \n...                        ...                         ...  \n48837                      0.0                         0.0  \n48838                      0.0                         0.0  \n48839                      0.0                         0.0  \n48840                      0.0                         0.0  \n48841                      0.0                         0.0  \n\n[48842 rows x 108 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>fnlwgt</th>\n      <th>education-num</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>workclass_ ?</th>\n      <th>workclass_ Federal-gov</th>\n      <th>workclass_ Local-gov</th>\n      <th>workclass_ Never-worked</th>\n      <th>...</th>\n      <th>native-country_ Portugal</th>\n      <th>native-country_ Puerto-Rico</th>\n      <th>native-country_ Scotland</th>\n      <th>native-country_ South</th>\n      <th>native-country_ Taiwan</th>\n      <th>native-country_ Thailand</th>\n      <th>native-country_ Trinadad&amp;Tobago</th>\n      <th>native-country_ United-States</th>\n      <th>native-country_ Vietnam</th>\n      <th>native-country_ Yugoslavia</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>0.044131</td>\n      <td>0.800000</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>0.048052</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>0.137581</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>0.150486</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>0.220635</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>0.137428</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0.209130</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>0.245379</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>0.048444</td>\n      <td>0.800000</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>0.114919</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 108 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_feature(df, feature_to_encode):\n",
    "    dummies = pd.get_dummies(df[[feature_to_encode]], dtype=float)\n",
    "    result_df = pd.concat([df, dummies], axis=1)\n",
    "    result_df.drop(columns=feature_to_encode, inplace=True)\n",
    "    return result_df\n",
    "\n",
    "df_one_hot = og_df.copy()\n",
    "df_one_hot.drop(columns=\"class\", inplace=True)\n",
    "df_one_hot[cont_cols] = MinMaxScaler().fit_transform(df_one_hot[cont_cols])\n",
    "for col in cat_cols:\n",
    "    df_one_hot = encode_feature(df_one_hot, col)\n",
    "df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7165758977928832"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(df_one_hot)\n",
    "kmeans_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "kmeans_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.615442Z",
     "end_time": "2023-07-06T00:22:12.746857Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1325918200579342"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "kmeans_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.742845Z",
     "end_time": "2023-07-06T00:22:12.800149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# no_target_df = og_df.drop(columns=\"class\")\n",
    "# distance_matrix = gower_matrix_duped(no_target_df)\n",
    "# distance_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.798446Z",
     "end_time": "2023-07-06T00:22:12.803668Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# gower_agglo = AgglomerativeClustering(n_clusters=2, metric=\"precomputed\", linkage=\"single\").fit_predict(distance_matrix)\n",
    "# gower_agglo_acc = cluster_accuracy(gower_agglo, og_df[\"class\"].to_numpy())\n",
    "# gower_agglo_acc\n",
    "# linkage=average: 0.7602882764833545\n",
    "# linkage=single: 0.760697760124483"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.801156Z",
     "end_time": "2023-07-06T00:22:12.810484Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# gower_agglo_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), gower_agglo)\n",
    "# gower_agglo_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.811484Z",
     "end_time": "2023-07-06T00:22:12.819335Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.820336Z",
     "end_time": "2023-07-06T00:22:12.871480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(9, 5), (16, 8), (7, 4), (15, 8), (6, 3), (5, 3), (2, 2), (42, 21)]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(df[col].nunique(), min(50, max(2, (df[col].nunique()+1) // 2))) for col in df[cat_cols]]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.837066Z",
     "end_time": "2023-07-06T00:22:12.872480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "48842"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CensusIncomeDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "    \n",
    "dataset = CensusIncomeDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-06T00:22:12.851657Z",
     "end_time": "2023-07-06T00:23:29.576830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 0.877130\n",
      "epoch: 2/100, loss = 0.604075\n",
      "epoch: 3/100, loss = 0.574528\n",
      "epoch: 4/100, loss = 0.552456\n",
      "epoch: 5/100, loss = 0.531077\n",
      "epoch: 6/100, loss = 0.513807\n",
      "epoch: 7/100, loss = 0.486766\n",
      "epoch: 8/100, loss = 0.455893\n",
      "epoch: 9/100, loss = 0.436293\n",
      "epoch: 10/100, loss = 0.427778\n",
      "epoch: 11/100, loss = 0.420987\n",
      "epoch: 12/100, loss = 0.414452\n",
      "epoch: 13/100, loss = 0.410343\n",
      "epoch: 14/100, loss = 0.406055\n",
      "epoch: 15/100, loss = 0.403028\n",
      "epoch: 16/100, loss = 0.399987\n",
      "epoch: 17/100, loss = 0.397938\n",
      "epoch: 18/100, loss = 0.396382\n",
      "epoch: 19/100, loss = 0.394773\n",
      "epoch: 20/100, loss = 0.394064\n",
      "epoch: 21/100, loss = 0.393135\n",
      "epoch: 22/100, loss = 0.392202\n",
      "epoch: 23/100, loss = 0.391937\n",
      "epoch: 24/100, loss = 0.392086\n",
      "epoch: 25/100, loss = 0.393049\n",
      "epoch: 26/100, loss = 0.394932\n",
      "epoch: 27/100, loss = 0.397575\n",
      "epoch: 28/100, loss = 0.400191\n",
      "epoch: 29/100, loss = 0.402559\n",
      "epoch: 30/100, loss = 0.403377\n",
      "epoch: 31/100, loss = 0.401913\n",
      "epoch: 32/100, loss = 0.397848\n",
      "epoch: 33/100, loss = 0.397073\n",
      "epoch: 34/100, loss = 0.396466\n",
      "epoch: 35/100, loss = 0.395655\n",
      "epoch: 36/100, loss = 0.396032\n",
      "epoch: 37/100, loss = 0.396093\n",
      "epoch: 38/100, loss = 0.395306\n",
      "epoch: 39/100, loss = 0.394793\n",
      "epoch: 40/100, loss = 0.394012\n",
      "epoch: 41/100, loss = 0.392330\n",
      "epoch: 42/100, loss = 0.392270\n",
      "epoch: 43/100, loss = 0.391888\n",
      "epoch: 44/100, loss = 0.391502\n",
      "epoch: 45/100, loss = 0.391233\n",
      "epoch: 46/100, loss = 0.391428\n",
      "epoch: 47/100, loss = 0.391631\n",
      "epoch: 48/100, loss = 0.391984\n",
      "epoch: 49/100, loss = 0.392594\n",
      "epoch: 50/100, loss = 0.392955\n",
      "epoch: 51/100, loss = 0.392947\n",
      "epoch: 52/100, loss = 0.393481\n",
      "epoch: 53/100, loss = 0.393978\n",
      "epoch: 54/100, loss = 0.394806\n",
      "epoch: 55/100, loss = 0.395457\n",
      "epoch: 56/100, loss = 0.396364\n",
      "epoch: 57/100, loss = 0.397302\n",
      "epoch: 58/100, loss = 0.397432\n",
      "epoch: 59/100, loss = 0.397687\n",
      "epoch: 60/100, loss = 0.398539\n",
      "epoch: 61/100, loss = 0.399892\n",
      "epoch: 62/100, loss = 0.401024\n",
      "epoch: 63/100, loss = 0.401611\n",
      "epoch: 64/100, loss = 0.402075\n",
      "epoch: 65/100, loss = 0.402483\n",
      "epoch: 66/100, loss = 0.402766\n",
      "epoch: 67/100, loss = 0.403372\n",
      "epoch: 68/100, loss = 0.403925\n",
      "epoch: 69/100, loss = 0.404452\n",
      "epoch: 70/100, loss = 0.405749\n",
      "epoch: 71/100, loss = 0.406439\n",
      "epoch: 72/100, loss = 0.407412\n",
      "epoch: 73/100, loss = 0.408257\n",
      "epoch: 74/100, loss = 0.409027\n",
      "epoch: 75/100, loss = 0.409827\n",
      "epoch: 76/100, loss = 0.410644\n",
      "epoch: 77/100, loss = 0.410689\n",
      "epoch: 78/100, loss = 0.412173\n",
      "epoch: 79/100, loss = 0.413225\n",
      "epoch: 80/100, loss = 0.413554\n",
      "epoch: 81/100, loss = 0.414426\n",
      "epoch: 82/100, loss = 0.415401\n",
      "epoch: 83/100, loss = 0.416938\n",
      "epoch: 84/100, loss = 0.417604\n",
      "epoch: 85/100, loss = 0.418338\n",
      "epoch: 86/100, loss = 0.419095\n",
      "epoch: 87/100, loss = 0.419819\n",
      "epoch: 88/100, loss = 0.420532\n",
      "epoch: 89/100, loss = 0.421766\n",
      "epoch: 90/100, loss = 0.422618\n",
      "epoch: 91/100, loss = 0.423498\n",
      "epoch: 92/100, loss = 0.424205\n",
      "epoch: 93/100, loss = 0.425488\n",
      "epoch: 94/100, loss = 0.426743\n",
      "epoch: 95/100, loss = 0.427142\n",
      "epoch: 96/100, loss = 0.428349\n",
      "epoch: 97/100, loss = 0.429677\n",
      "epoch: 98/100, loss = 0.430765\n",
      "epoch: 99/100, loss = 0.432811\n",
      "epoch: 100/100, loss = 0.434149\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderOnlyCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, n_emb),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "model = AttentionModelDecoderOnlyCat()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, model.last_target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.85494751, 0.94251174, 0.16518661, ..., 0.02174022, 0.        ,\n        0.39795918],\n       [0.84306568, 0.03665686, 0.42526764, ..., 0.        , 0.        ,\n        0.12244898],\n       [0.11221159, 0.8301397 , 0.31617841, ..., 0.        , 0.        ,\n        0.39795918],\n       ...,\n       [0.86946476, 0.04501979, 0.3015568 , ..., 0.        , 0.        ,\n        0.5       ],\n       [0.87331426, 0.77441919, 0.2433614 , ..., 0.05455055, 0.        ,\n        0.39795918],\n       [0.8950839 , 0.04445706, 0.6727984 , ..., 0.        , 0.        ,\n        0.60204082]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "cat_features = model.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, df[cont_cols].values), 1)\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:23:29.578831Z",
     "end_time": "2023-07-06T00:23:37.092836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6677040252241923"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "deep_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "deep_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:23:37.090413Z",
     "end_time": "2023-07-06T00:23:37.170931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "0.08647618787878508"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "deep_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:23:37.170931Z",
     "end_time": "2023-07-06T00:23:37.226087Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 1.139552\n",
      "epoch: 2/100, loss = 0.866271\n",
      "epoch: 3/100, loss = 0.821793\n",
      "epoch: 4/100, loss = 0.799370\n",
      "epoch: 5/100, loss = 0.777468\n",
      "epoch: 6/100, loss = 0.740899\n",
      "epoch: 7/100, loss = 0.703400\n",
      "epoch: 8/100, loss = 0.676251\n",
      "epoch: 9/100, loss = 0.655495\n",
      "epoch: 10/100, loss = 0.638118\n",
      "epoch: 11/100, loss = 0.622059\n",
      "epoch: 12/100, loss = 0.609821\n",
      "epoch: 13/100, loss = 0.597719\n",
      "epoch: 14/100, loss = 0.586438\n",
      "epoch: 15/100, loss = 0.573809\n",
      "epoch: 16/100, loss = 0.563828\n",
      "epoch: 17/100, loss = 0.555599\n",
      "epoch: 18/100, loss = 0.549672\n",
      "epoch: 19/100, loss = 0.545796\n",
      "epoch: 20/100, loss = 0.542190\n",
      "epoch: 21/100, loss = 0.539683\n",
      "epoch: 22/100, loss = 0.535772\n",
      "epoch: 23/100, loss = 0.533745\n",
      "epoch: 24/100, loss = 0.531797\n",
      "epoch: 25/100, loss = 0.530744\n",
      "epoch: 26/100, loss = 0.530430\n",
      "epoch: 27/100, loss = 0.528593\n",
      "epoch: 28/100, loss = 0.526576\n",
      "epoch: 29/100, loss = 0.525135\n",
      "epoch: 30/100, loss = 0.523559\n",
      "epoch: 31/100, loss = 0.522353\n",
      "epoch: 32/100, loss = 0.521910\n",
      "epoch: 33/100, loss = 0.520373\n",
      "epoch: 34/100, loss = 0.519539\n",
      "epoch: 35/100, loss = 0.519168\n",
      "epoch: 36/100, loss = 0.519227\n",
      "epoch: 37/100, loss = 0.517814\n",
      "epoch: 38/100, loss = 0.517995\n",
      "epoch: 39/100, loss = 0.518338\n",
      "epoch: 40/100, loss = 0.519065\n",
      "epoch: 41/100, loss = 0.519493\n",
      "epoch: 42/100, loss = 0.519926\n",
      "epoch: 43/100, loss = 0.520838\n",
      "epoch: 44/100, loss = 0.521369\n",
      "epoch: 45/100, loss = 0.521715\n",
      "epoch: 46/100, loss = 0.523136\n",
      "epoch: 47/100, loss = 0.522155\n",
      "epoch: 48/100, loss = 0.522434\n",
      "epoch: 49/100, loss = 0.522950\n",
      "epoch: 50/100, loss = 0.523771\n",
      "epoch: 51/100, loss = 0.524291\n",
      "epoch: 52/100, loss = 0.523411\n",
      "epoch: 53/100, loss = 0.525135\n",
      "epoch: 54/100, loss = 0.526919\n",
      "epoch: 55/100, loss = 0.528480\n",
      "epoch: 56/100, loss = 0.529577\n",
      "epoch: 57/100, loss = 0.529410\n",
      "epoch: 58/100, loss = 0.529718\n",
      "epoch: 59/100, loss = 0.530799\n",
      "epoch: 60/100, loss = 0.531666\n",
      "epoch: 61/100, loss = 0.532227\n",
      "epoch: 62/100, loss = 0.533506\n",
      "epoch: 63/100, loss = 0.534808\n",
      "epoch: 64/100, loss = 0.535493\n",
      "epoch: 65/100, loss = 0.536068\n",
      "epoch: 66/100, loss = 0.535887\n",
      "epoch: 67/100, loss = 0.536867\n",
      "epoch: 68/100, loss = 0.537636\n",
      "epoch: 69/100, loss = 0.539043\n",
      "epoch: 70/100, loss = 0.538751\n",
      "epoch: 71/100, loss = 0.539764\n",
      "epoch: 72/100, loss = 0.540547\n",
      "epoch: 73/100, loss = 0.542270\n",
      "epoch: 74/100, loss = 0.542884\n",
      "epoch: 75/100, loss = 0.543701\n",
      "epoch: 76/100, loss = 0.544387\n",
      "epoch: 77/100, loss = 0.545169\n",
      "epoch: 78/100, loss = 0.546536\n",
      "epoch: 79/100, loss = 0.547202\n",
      "epoch: 80/100, loss = 0.548808\n",
      "epoch: 81/100, loss = 0.550933\n",
      "epoch: 82/100, loss = 0.551371\n",
      "epoch: 83/100, loss = 0.553503\n",
      "epoch: 84/100, loss = 0.554130\n",
      "epoch: 85/100, loss = 0.555365\n",
      "epoch: 86/100, loss = 0.557102\n",
      "epoch: 87/100, loss = 0.556959\n",
      "epoch: 88/100, loss = 0.558135\n",
      "epoch: 89/100, loss = 0.559866\n",
      "epoch: 90/100, loss = 0.562435\n",
      "epoch: 91/100, loss = 0.563349\n",
      "epoch: 92/100, loss = 0.565382\n",
      "epoch: 93/100, loss = 0.566955\n",
      "epoch: 94/100, loss = 0.566788\n",
      "epoch: 95/100, loss = 0.568873\n",
      "epoch: 96/100, loss = 0.570742\n",
      "epoch: 97/100, loss = 0.571391\n",
      "epoch: 98/100, loss = 0.572943\n",
      "epoch: 99/100, loss = 0.575244\n",
      "epoch: 100/100, loss = 0.575418\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderAllCols(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, in_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "all_cols_model = AttentionModelDecoderAllCols()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(all_cols_model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    all_cols_model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = all_cols_model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, torch.cat((all_cols_model.last_target, x_cont), 1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:23:37.225088Z",
     "end_time": "2023-07-06T00:24:54.801683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.94522315, 0.18745   , 0.7496655 , ..., 0.926956  , 0.73959196,\n        0.99683136],\n       [0.9870445 , 0.05297403, 0.22871901, ..., 0.4084474 , 0.01855831,\n        0.9575082 ],\n       [0.0382531 , 0.5314856 , 0.90828824, ..., 0.69764507, 0.66231346,\n        0.01178148],\n       ...,\n       [0.6658559 , 0.5523463 , 0.8665732 , ..., 0.7398805 , 0.09992711,\n        0.9698183 ],\n       [0.4806827 , 0.31868738, 0.93334615, ..., 0.6029834 , 0.4110806 ,\n        0.95378566],\n       [0.99550086, 0.03021652, 0.14750136, ..., 0.39120314, 0.02528557,\n        0.9745515 ]], dtype=float32)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "decoder_all_cols_features = all_cols_model.encode(cat, cont).detach().numpy()\n",
    "decoder_all_cols_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:24:54.802682Z",
     "end_time": "2023-07-06T00:25:02.196914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6294582531427869"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(decoder_all_cols_features)\n",
    "all_cols_acc = cluster_accuracy(all_cols_kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "all_cols_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:25:02.197914Z",
     "end_time": "2023-07-06T00:25:02.244376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "0.06302645856213651"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), all_cols_kmeans.labels_)\n",
    "all_cols_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:25:02.245380Z",
     "end_time": "2023-07-06T00:25:02.295584Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Accuracy       NMI\nKMeans                                              0.716576  0.132592\nGower + Agglomerative                               0.760698  0.000023\nDeep Attention KMeans, only Cat Cols reconstruc...  0.667704  0.086476\nDeep Attention KMeans, all Cols reconstructed i...  0.629458  0.063026",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>NMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>KMeans</th>\n      <td>0.716576</td>\n      <td>0.132592</td>\n    </tr>\n    <tr>\n      <th>Gower + Agglomerative</th>\n      <td>0.760698</td>\n      <td>0.000023</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, only Cat Cols reconstructed in Decoder</th>\n      <td>0.667704</td>\n      <td>0.086476</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, all Cols reconstructed in Decoder</th>\n      <td>0.629458</td>\n      <td>0.063026</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[kmeans_acc, kmeans_nmi], [0.760697760124483, 0.000023], [deep_acc, deep_nmi], [all_cols_acc, all_cols_nmi]], index=[\"KMeans\", \"Gower + Agglomerative\", \"Deep Attention KMeans, only Cat Cols reconstructed in Decoder\", \"Deep Attention KMeans, all Cols reconstructed in Decoder\"], columns=[\"Accuracy\", \"NMI\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-06T00:25:02.295584Z",
     "end_time": "2023-07-06T00:25:02.302713Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
