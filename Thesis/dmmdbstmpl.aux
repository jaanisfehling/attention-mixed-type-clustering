\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\babel@aux{english}{}
\citation{mixed_type_survey_2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{deep_neural_networks_tabular_data_survey}
\citation{tabnet}
\citation{tab_transformer}
\citation{ft_transformer}
\citation{attention_between_datapoints}
\citation{saint}
\citation{sadsc}
\citation{dagc}
\citation{cmfgn}
\citation{dlgamc}
\citation{dcn}
\citation{dec}
\citation{idec}
\citation{depict}
\citation{dec-da}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{uci_ml_rpo}
\citation{abalone}
\citation{auction_verification}
\citation{bank_marketing}
\citation{breast_cancer}
\citation{census_income}
\citation{credit_approval}
\citation{heart_disease}
\citation{abalone}
\citation{scikit_learn}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Foundation}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Methodology}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Datasets}{5}{}\protected@file@percent }
\newlabel{Datasets}{{3.1.1}{5}}
\citation{pytorch}
\citation{scipy}
\citation{scikit_learn}
\citation{scikit_learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Evaluation}{6}{}\protected@file@percent }
\newlabel{Evaluation}{{3.1.2}{6}}
\citation{kmeans}
\citation{kmeans_np_hard}
\citation{kmeans_np_hard}
\citation{kmeans_lloyd}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Classical Methods for clustering Mixed-Type data}{7}{}\protected@file@percent }
\newlabel{Classical Methods for clustering Mixed-Type data}{{3.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}k-means}{7}{}\protected@file@percent }
\newlabel{k-means}{{3.2.1}{7}}
\citation{pattern_recognition_machine_learning}
\citation{kmodes}
\citation{kmodes}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}k-modes}{8}{}\protected@file@percent }
\newlabel{k-modes}{{3.2.2}{8}}
\citation{kmodes}
\citation{kmodes}
\citation{kmodes}
\citation{kmodes}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}k-prototypes}{9}{}\protected@file@percent }
\citation{kmodes}
\citation{gower}
\citation{gower}
\citation{gower}
\citation{gower}
\citation{gower}
\citation{algorithms_for_clustering_data}
\citation{philip_ottaway}
\citation{algorithms_for_clustering_data}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Gower distance}{10}{}\protected@file@percent }
\citation{neural_network_1943}
\citation{perceptron}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration from the original paper by Gower \cite  {gower}. Score $s_{ijk}$ and quantity $\delta _{ijk}$ of a feature $k$ on two instances $x_i$ and $x_j$. Presence of a feature is denoted by "+" and absence by "-".}}{11}{}\protected@file@percent }
\newlabel{gower_dichotomous}{{3.1}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Deep Clustering}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Neural Networks}{11}{}\protected@file@percent }
\newlabel{Neural Networks}{{3.3.1}{11}}
\citation{hidden_layer_backprop}
\citation{neural_networks_pattern_recognition}
\citation{neural_networks_pattern_recognition}
\citation{activation_functions}
\citation{activation_functions}
\citation{neural_networks_pattern_recognition}
\citation{activation_functions}
\citation{activation_functions}
\citation{activation_functions}
\citation{activation_functions}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An illustration of the step function, the logistic sigmoid, the ReLU and the Leaky ReLU activation functions.}}{13}{}\protected@file@percent }
\newlabel{activation_functions}{{3.2}{13}}
\citation{neural_networks_pattern_recognition}
\citation{autoencoder}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Autoencoder}{14}{}\protected@file@percent }
\citation{dcn}
\citation{deep_clustering_survey}
\citation{dec}
\citation{comparing_DEC_and_DCN}
\citation{comparing_DEC_and_DCN}
\citation{dec}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustration of an autoencoder neural network.}}{15}{}\protected@file@percent }
\newlabel{autoencoder_illustration}{{3.3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Deep Clustering}{15}{}\protected@file@percent }
\newlabel{Deep Clustering}{{3.3.3}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Illustration of DEC by Xie et al. \cite  {dec}.}}{16}{}\protected@file@percent }
\newlabel{dec}{{3.4}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Illustration from Li \cite  {comparing_DEC_and_DCN} on the distributions $Q$ and $P$ on two cluster centroids $\mu _1$ and $\mu _2$. It is notable that $P$ is more concentrated around the cluster centers, but still derived from $Q$.}}{16}{}\protected@file@percent }
\newlabel{p-and-q-distributions}{{3.5}{16}}
\citation{dec}
\citation{idec}
\citation{idec}
\citation{dcn}
\citation{dcn}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Illustration of IDEC by Guo et al. \cite  {idec}.}}{18}{}\protected@file@percent }
\newlabel{idec}{{3.6}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Illustration of DCN by Yang et al. \cite  {dcn}.}}{18}{}\protected@file@percent }
\newlabel{dcn}{{3.7}{18}}
\citation{kmodes}
\citation{kmodes}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Attention in Mixed-Type Clustering}{20}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Autoencoder and k-means}{20}{}\protected@file@percent }
\newlabel{Autoencoder and k-means}{{4.1}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Column Embeddings}{21}{}\protected@file@percent }
\newlabel{Column Embeddings}{{4.2}{21}}
\citation{bahdanau}
\citation{attention_is_all_you_need}
\citation{transformers_from_scratch}
\citation{transformers_from_scratch}
\citation{attention_is_all_you_need}
\citation{tab_transformer}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Attention}{22}{}\protected@file@percent }
\newlabel{Attention}{{4.3}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Architecture of the self-attention autoencoder network.}}{23}{}\protected@file@percent }
\newlabel{attention_architecture}{{4.1}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Illustration of the attention mechanism on calculating the first output vector, inspired by Bloem \cite  {transformers_from_scratch}. q1, k1, v1 represent the vectors at index 1 of Queries $Q$, Keys $K$ and Values $V$ respectively. The softmax function over the weights is not part of this illustration.}}{24}{}\protected@file@percent }
\newlabel{attention_illustrated}{{4.2}{24}}
\citation{neural_networks_pattern_recognition}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{attention_is_all_you_need}
\citation{layer_normalization}
\citation{tab_transformer}
\citation{tab_transformer}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Transformer}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}FT-Transformer}{26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Architecture of the Transformer autoencoder network.}}{27}{}\protected@file@percent }
\newlabel{transformer_autoencoder}{{4.3}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Architecture of the FT-Transformer autoencoder network.}}{29}{}\protected@file@percent }
\newlabel{ft_transformer_autoencoder}{{4.4}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments}{30}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Experiments}{{5}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Comparison of classical Clustering Methods}{30}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of Normalized Mutual Information of various classical methods on clustering mixed-type datasets.}}{30}{}\protected@file@percent }
\newlabel{classical_comparison_nmi}{{5.1}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of accuracy of various classical methods on clustering mixed-type datasets.}}{31}{}\protected@file@percent }
\newlabel{classical_comparison_acc}{{5.2}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Number of instances of each target class of each dataset used.}}{31}{}\protected@file@percent }
\newlabel{class_imbalance}{{5.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparison of Normalized Mutual Information of various autoencoder architectures combined with k-means on clustering mixed-type datasets.}}{32}{}\protected@file@percent }
\newlabel{ae_architecture_comparison_nmi}{{5.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison of accuracy of various autoencoder architectures combined with k-means on clustering mixed-type datasets.}}{32}{}\protected@file@percent }
\newlabel{ae_architecture_comparison_acc}{{5.5}{32}}
\citation{tab_transformer}
\citation{ft_transformer}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison of Normalized Mutual Information between an autoencoder with categorical embeddings, an autoencoder using the attention mechanism, a 6-layer Transformer with an autoencoder and a 6-layer FT-Transformer with an autoencoder on clustering mixed-type datasets using k-means.}}{33}{}\protected@file@percent }
\newlabel{attention_comparison_nmi}{{5.6}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison of accuracy between an autoencoder with categorical embeddings, an autoencoder using the attention mechanism, a 6-layer Transformer with an autoencoder and a 6-layer FT-Transformer with an autoencoder on clustering mixed-type datasets using k-means.}}{34}{}\protected@file@percent }
\newlabel{attention_comparison_acc}{{5.7}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Comparison of Normalized Mutual Information between an autoencoder with categorical embeddings, an autoencoder with attention, a 6-layer Transformer with an autoencoder and a 6-layer FT-Transformer with an autoencoder on clustering mixed-type datasets. DEC, IDEC and DCN were used as clustering methods.}}{35}{}\protected@file@percent }
\newlabel{deep_comparison_nmi}{{5.8}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison of accuracy between an autoencoder with categorical embeddings, an autoencoder with attention, a 6-layer Transformer with an autoencoder and a 6-layer FT-Transformer with an autoencoder on clustering mixed-type datasets. DEC, IDEC and DCN were used as clustering methods.}}{36}{}\protected@file@percent }
\newlabel{deep_comparison_acc}{{5.9}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Best Normalized Mutual Information scores (in parenthesis) across all clustering methods. Classical methods are colored in green, methods using an autoencoder with k-means are colored cyan, methods using an autoencoder and attention or a transformer mechanism are colored yellow and methods using DEC, IDEC or DCN are colored magenta.}}{37}{}\protected@file@percent }
\newlabel{final_comparison_nmi}{{5.10}{37}}
\bibstyle{plain}
\bibdata{dmmdbstmpl}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{38}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{mixed_type_survey_2019}{1}
\bibcite{tabnet}{2}
\bibcite{layer_normalization}{3}
\bibcite{bahdanau}{4}
\bibcite{autoencoder}{5}
\bibcite{neural_networks_pattern_recognition}{6}
\bibcite{pattern_recognition_machine_learning}{7}
\bibcite{transformers_from_scratch}{8}
\bibcite{deep_neural_networks_tabular_data_survey}{9}
\bibcite{sadsc}{10}
\bibcite{depict}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{39}{}\protected@file@percent }
\bibcite{activation_functions}{12}
\bibcite{ft_transformer}{13}
\bibcite{gower}{14}
\bibcite{idec}{15}
\bibcite{dec-da}{16}
\bibcite{cmfgn}{17}
\bibcite{tab_transformer}{18}
\bibcite{kmodes}{19}
\bibcite{algorithms_for_clustering_data}{20}
\bibcite{heart_disease}{21}
\bibcite{census_income}{22}
\bibcite{attention_between_datapoints}{23}
\bibcite{comparing_DEC_and_DCN}{24}
\bibcite{dlgamc}{25}
\bibcite{kmeans_lloyd}{26}
\bibcite{kmeans}{27}
\bibcite{kmeans_np_hard}{28}
\bibcite{uci_ml_rpo}{29}
\bibcite{neural_network_1943}{30}
\bibcite{bank_marketing}{31}
\bibcite{abalone}{32}
\bibcite{auction_verification}{33}
\bibcite{pytorch}{34}
\bibcite{scikit_learn}{35}
\bibcite{dagc}{36}
\bibcite{philip_ottaway}{37}
\bibcite{credit_approval}{38}
\bibcite{perceptron}{39}
\bibcite{hidden_layer_backprop}{40}
\bibcite{saint}{41}
\bibcite{attention_is_all_you_need}{42}
\bibcite{scipy}{43}
\bibcite{breast_cancer}{44}
\bibcite{dec}{45}
\bibcite{dcn}{46}
\bibcite{deep_clustering_survey}{47}
\gdef \@abspage@last{46}
