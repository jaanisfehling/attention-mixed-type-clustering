\chapter{Introduction}


\section{Introduction}

Clustering is an unsupervised machine learning method that groups similar observations together. Due to its ability to find patterns in an unlabeled dataset, its an essential task in Data Mining and Knowledge Discovery. A \textit{cluster} is a group of similar observations that belongs to a \textit{centroid} (center point of a cluster). Distance-based clustering algorithms use distance measures such as Euclidean distance to calculate the similarity of datapoints. Hierarchical methods partition the observations and merge (agglomerative) or split them into bigger or smaller clusters. Many other methods exist, but this work focuses on methods for clustering \textit{mixed-type} data. \cite{mixed_type_survey_2019}

\section{k-means}

The most well known distance-based clustering method is k-means \cite{kmeans}. The goal is defined as follows: Suppose we have a finite set of $n$ observations $S=\{p_1, p_2, ..., p_n\} \in \R^m$ for a dataset with $m$ features, the target of k-means is to find optimal centroids $B=\{b_1, b_2, ..., b_k\} \subseteq \R^m$ for a given $k (\leq n) \in \N$ that minimize the sum of the squared Euclidean distance of each point in $S$ to its nearest centroid. Formally
$$\sum_{i=1}^n  d(p_i, B)$$
has to be minimized, where $d$ is the Euclidean distance from a point $p_i \in S$ to the nearest centroid in $B$ \cite{kmeans_np_hard}: 
$$d(p_i, B) = min_{1 \leq j \leq k} d(p_i, b_j)$$
The Euclidean distance between two points $p$ and $q$ in an $n$-dimensional Euclidean space is defined as 
$$d(p, q) = || p - q || = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2}$$
Finding the optimal centroids is a NP-hard problem, even for $d=2$, as shown by Mahajan et al. \cite{kmeans_np_hard}. The most common algorithm used for the k-means problem is a iterative refinement technique proposed by Lloyd \cite{kmeans_lloyd}. It is defined as follows:
\begin{enumerate} 
	\item Randomly set $k$ initial cluster centroids $b_1^{(1)}, ..., b_k^{(1)}$.
	\item Assign each obseration $p_i$ to the nearest centroid using squared Euclidean distance. This splits our observations into $S$ into $k$ sets $\{S_1^{(t)}, ..., S_k^{(t)}\}$.
	\item Recalculate the optimal position of each centroid using the mean distance to each observation assigned to the centroid: 
$$b_i^{(t+1)} = \frac{1}{|S_i^{(t)}|} \sum_{p_j \in S_i^{(t)}} p_j$$
	\item Repeat steps 2. and 3. until the centroid assignments no longer change.
\end{enumerate}

\section{Mixed-type data}

In many real-world scenarios, besides continuous, numerical data, \textit{categorial} data exists. While Euclidean distance or other distance measures work well with continuous data, categorial data is different. Suppose we have categories $\{A, B, C\}$ of a given feature, we would encode them into numeric values to allow for computation of a distance measure:
$$\{A, B, C\} \equiv \{1, 2, 3\}$$
While $A$ and $C$ can share the same semantic similarity as $A$ and $B$, numerically category $A$ and Category $C$ are now $|1-3| = 2$ apart, while Category $A$ and $B$ are only $|1-2|=1$ apart. During clustering, this could lead to observations being assigned to centroids based on a wrong distance assumption.

A possible solution is to use \textit{one-hot encoding}, also known as \textit{dummy coding} in classical statistics. One-hot encoding turns a discrete feature containing $k$ mutually exclusive categories into a vector $x$ of length $k$, in which only one of the elements $x_k$ equals 1 and all remaining elements equal 0 \cite{bishop_2006}. For an observation $B$ of a feature having $k=3$ separate categories $\{A, B, C\}$, the one-hot vector $x$ would be represented by $x = (0, 1, 0)^{\intercal}$.

\subsection{k-modes} \label{k-modes}

According to Huang \cite{kmodes}, one-hot encoding has two drawbacks:
\begin{enumerate} 
	\item In real-world applications, categorial features with hundreds or thousands of categories are encountered. This would result in a large number of binary features in the one-hot encoded representation, which will increase cost and space of computation.
	\item The centroid value of a certain one-hot encoded feature, given by a real value between 0 and 1, cannot indicate the characteristics of the according cluster, since the feature only describes the presence or absence of one category.
\end{enumerate}
Therefore, Huang \cite{kmodes} proposed using the Kronecker-Delta as a dissimilarity measure between multiple categorial columns. Formally, if we have two observations $X$ and $Y$ of a dataset with $m$ categorial features, $d_1$ will count the number of mismatches between the categorial features of both instances, defined as
$$d_1(X, Y) = \sum^m_{j=1} \delta (x_j, y_j)$$
where the Kronecker delta $\delta (x_j, y_j)$ is defined as
$$\delta (x_j, y_j) = 
\begin{cases}
    0, & (x_j = y_j)\\
    1, & (x_j \neq y_j)
\end{cases}$$
If we have a finite set of $n$ observations $S=\{p_1, p_2, ..., p_n\} \in \R^m$ for a dataset with $m$ categorial features, the goal of \textit{k-modes} \cite{kmodes} is to find optimal \textit{modes} $B=\{b_1, b_2, ..., b_k\} \subseteq \R^m$ for a given $k (\leq n) \in \N$ that minimize
$$\sum_{i=1}^n  d_1(p_i, B)$$
where
$$d_1(p_i, B) = min_{1 \leq l \leq k} \sum^m_{j=1} \delta (p_{i,j}, b_{l,j})$$
Similar to k-means, we can use an iterative algorithm for efficient computation \cite{kmodes}:
\begin{enumerate} 
	\item Randonly choose $k$ observations from the dataset as initial modes for the clusters.
	\item Assign each observation to their nearest mode using the proposed dissimilarity measure one by one and update the mode of each cluster after each assignment.
	\item Test if each observation still belongs to its assigned mode, i.e. if each observation is assigned to its nearest mode. If the observation would belong to a different mode, reassign the observation and update the modes of both clusters.
	\item Repeat step 3. until the mode assignments no longer change.
\end{enumerate}

\subsection{k-prototypes}

As proposed by Huang \cite{kmodes}, it is straightforward to combine the k-means and k-modes algorithms into the \textit{k-prototypes} algorithm, which can be used to cluster \textit{mixed-type} data (consisting of numerical, continuous and categorial features). The dissimilarity between two observations $X$ and $Y$ with features $A^r_1, A^r_2, ..., A^r_s, A_{s+1}^c, ..., A^c_m$, where features $A^r_1, ..., A^r_s$ are continuous and features $A_{s+1}^c, ..., A^c_m$ are categorial, is defined as
$$d_2(X,Y) = \sum^s_{j=1}(x_j - y_j)^2 + \gamma \sum^m_{j=s+1}\delta(x_j, y_j)$$
The first part of the equation is the Euclidean distance as used in k-means, while the second part is taken from the k-modes algorithm. Huang \cite{kmodes} states: "The weight $\gamma$ is used to avoid favouring either type of attribute".

Again, we need to find $k$ optimal centroids $B=\{b_1, b_2, ..., b_k\}$ and therefore have to minimize
$$\sum_{i=1}^n  d_2(p_i, B)$$
where
$$d_2(p_i, B) = min_{1 \leq l \leq k} \sum^s_{j=1}(p_{i,j} - b_{l,j})^2 + \gamma \sum^m_{j=s+1}\delta(p_{i,j}, b_{l,j})$$
We can minimize both distance measures at the same time since they are nonnegative. Therefore, we can use the same algorithm as defined in \ref{k-modes}. \cite{kmodes}

\section{Methodology}

In this work we use 8 mixed-type datasets from the UC Irvine Machine Learning Repository \cite{uci_ml_rpo}. All observations containing missing values were removed. Categorial columns were standardized by removing the mean and scaling to unit variance, using the scikit-learn Python API \cite{scikit_learn}. The standardized score $z$ of a sample $x$ from a feature is calculated as:
$$z = \frac{(x-\mu)}{\sigma}$$
where $\mu$ is the mean of the samples $x_1, ...,x_N$ from a feature of length $N$, defined as
$$\mu = \frac{1}{N} \sum^{N}_{i=1} x_i$$
and $\sigma$ is the standard deviation of the samples of a feature, defined as
$$\sigma = \sqrt{\frac{1}{N} \sum^{N}_{i=1}(x_i - \mu)^2}$$




\begin{figure}
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
	\hline
	&Naive k-means&k-means one-hot&k-prototypes&Gower distance\\ \hline
	Soybean Disease&0.679950&0.674068&\bf{0.694658}&0.669526\\ \hline
	Heart Disease&\bf{0.198871}&0.193008&0.153387&0.140792\\ \hline
	Breast Cancer&\bf{0.748214}&0.734909&0.587960&0.537113\\ \hline
	Bank Marketing&0.017640&\bf{0.029666}&0.017566&0.000356\\ \hline
	Census Income&0.072767&\bf{0.146271}&0.022475&0.000507\\ \hline
	Credit Approval&\bf{0.225376}&0.161375&0.114664&0.003465\\
	\hline
    \end{tabular}
\end{center}
\caption{Comparsion of Normalized Mutual Information (NMI) of classical methods on clustering mixed-type data. Naive k-means and k-means with one-hot encoding where calculated 100 times and the average NMI was taken. After calculating the Gower distance matrix, Agglomerative Clustering with average linkage between sets of observation was used. Due to high runtime and memory consumption, the "Bank Marketing" and "Census Income" datasets where downsampled to the first 2000 instances.}
\label{classical_comparison}
\end{figure}











