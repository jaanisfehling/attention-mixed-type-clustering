{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.529942Z",
     "end_time": "2023-07-02T16:24:29.543495Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from gower_duped import gower_matrix as gower_matrix_duped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.531942Z",
     "end_time": "2023-07-02T16:24:29.544493Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_accuracy(y_pred, y_true):\n",
    "    # We need to map the labels to our cluster labels\n",
    "    # This is a linear assignment problem on a bipartite graph\n",
    "    k = max(len(np.unique(y_pred)), len(np.unique(y_pred)))\n",
    "    cost_matrix = np.zeros((k, k))\n",
    "    for i in range(y_pred.size):\n",
    "        cost_matrix[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix.max() - cost_matrix)\n",
    "    return cost_matrix[row_ind, col_ind].sum() / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.542494Z",
     "end_time": "2023-07-02T16:24:29.618195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       age          workclass  fnlwgt   education  education-num   \n0       39          State-gov   77516   Bachelors             13  \\\n1       50   Self-emp-not-inc   83311   Bachelors             13   \n2       38            Private  215646     HS-grad              9   \n3       53            Private  234721        11th              7   \n4       28            Private  338409   Bachelors             13   \n...    ...                ...     ...         ...            ...   \n48837   39            Private  215419   Bachelors             13   \n48838   64                  ?  321403     HS-grad              9   \n48839   38            Private  374983   Bachelors             13   \n48840   44            Private   83891   Bachelors             13   \n48841   35       Self-emp-inc  182148   Bachelors             13   \n\n            marital-status          occupation     relationship   \n0            Never-married        Adm-clerical    Not-in-family  \\\n1       Married-civ-spouse     Exec-managerial          Husband   \n2                 Divorced   Handlers-cleaners    Not-in-family   \n3       Married-civ-spouse   Handlers-cleaners          Husband   \n4       Married-civ-spouse      Prof-specialty             Wife   \n...                    ...                 ...              ...   \n48837             Divorced      Prof-specialty    Not-in-family   \n48838              Widowed                   ?   Other-relative   \n48839   Married-civ-spouse      Prof-specialty          Husband   \n48840             Divorced        Adm-clerical        Own-child   \n48841   Married-civ-spouse     Exec-managerial          Husband   \n\n                      race      sex  capital-gain  capital-loss   \n0                    White     Male          2174             0  \\\n1                    White     Male             0             0   \n2                    White     Male             0             0   \n3                    Black     Male             0             0   \n4                    Black   Female             0             0   \n...                    ...      ...           ...           ...   \n48837                White   Female             0             0   \n48838                Black     Male             0             0   \n48839                White     Male             0             0   \n48840   Asian-Pac-Islander     Male          5455             0   \n48841                White     Male             0             0   \n\n       hours-per-week  native-country    class  \n0                  40   United-States    <=50K  \n1                  13   United-States    <=50K  \n2                  40   United-States    <=50K  \n3                  40   United-States    <=50K  \n4                  40            Cuba    <=50K  \n...               ...             ...      ...  \n48837              36   United-States   <=50K.  \n48838              40   United-States   <=50K.  \n48839              50   United-States   <=50K.  \n48840              40   United-States   <=50K.  \n48841              60   United-States    >50K.  \n\n[48842 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>39</td>\n      <td>Private</td>\n      <td>215419</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Prof-specialty</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>64</td>\n      <td>?</td>\n      <td>321403</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Widowed</td>\n      <td>?</td>\n      <td>Other-relative</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>374983</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>83891</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>Asian-Pac-Islander</td>\n      <td>Male</td>\n      <td>5455</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>35</td>\n      <td>Self-emp-inc</td>\n      <td>182148</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>United-States</td>\n      <td>&gt;50K.</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df = pd.read_csv(\"datasets/census_income.csv\")\n",
    "og_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7607182343065395"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df.loc[(og_df[\"class\"] == \" <=50K.\") | (og_df[\"class\"] == \" <=50K\"), \"class\"] = 0\n",
    "og_df.loc[(og_df[\"class\"] == \" >50K.\") | (og_df[\"class\"] == \" >50K\"), \"class\"] = 1\n",
    "# Probability of most common class\n",
    "og_df[\"class\"].value_counts().max()/og_df[\"class\"].count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.619199Z",
     "end_time": "2023-07-02T16:24:29.637032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.638604Z",
     "end_time": "2023-07-02T16:24:29.640114Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "cont_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.641114Z",
     "end_time": "2023-07-02T16:24:29.731851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age  workclass    fnlwgt  education  education-num   \n0      0.301370          7  0.044131          9       0.800000  \\\n1      0.452055          6  0.048052          9       0.800000   \n2      0.287671          4  0.137581         11       0.533333   \n3      0.493151          4  0.150486          1       0.400000   \n4      0.150685          4  0.220635          9       0.800000   \n...         ...        ...       ...        ...            ...   \n48837  0.301370          4  0.137428          9       0.800000   \n48838  0.643836          0  0.209130         11       0.533333   \n48839  0.287671          4  0.245379          9       0.800000   \n48840  0.369863          4  0.048444          9       0.800000   \n48841  0.246575          5  0.114919          9       0.800000   \n\n       marital-status  occupation  relationship  race  sex  capital-gain   \n0                   4           1             1     4    1      0.021740  \\\n1                   2           4             0     4    1      0.000000   \n2                   0           6             1     4    1      0.000000   \n3                   2           6             0     2    1      0.000000   \n4                   2          10             5     2    0      0.000000   \n...               ...         ...           ...   ...  ...           ...   \n48837               0          10             1     4    0      0.000000   \n48838               6           0             2     2    1      0.000000   \n48839               2          10             0     4    1      0.000000   \n48840               0           1             3     1    1      0.054551   \n48841               2           4             0     4    1      0.000000   \n\n       capital-loss  hours-per-week  native-country  \n0               0.0        0.397959              39  \n1               0.0        0.122449              39  \n2               0.0        0.397959              39  \n3               0.0        0.397959              39  \n4               0.0        0.397959               5  \n...             ...             ...             ...  \n48837           0.0        0.357143              39  \n48838           0.0        0.397959              39  \n48839           0.0        0.500000              39  \n48840           0.0        0.397959              39  \n48841           0.0        0.602041              39  \n\n[48842 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>7</td>\n      <td>0.044131</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>6</td>\n      <td>0.048052</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.137581</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>4</td>\n      <td>0.150486</td>\n      <td>1</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>4</td>\n      <td>0.220635</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>4</td>\n      <td>0.137428</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0</td>\n      <td>0.209130</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.245379</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>4</td>\n      <td>0.048444</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>5</td>\n      <td>0.114919</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = og_df.copy()\n",
    "df.drop(columns=\"class\", inplace=True)\n",
    "df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)\n",
    "df[cont_cols] = MinMaxScaler().fit_transform(df[cont_cols])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.703290Z",
     "end_time": "2023-07-02T16:24:29.884595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age    fnlwgt  education-num  capital-gain  capital-loss   \n0      0.301370  0.044131       0.800000      0.021740           0.0  \\\n1      0.452055  0.048052       0.800000      0.000000           0.0   \n2      0.287671  0.137581       0.533333      0.000000           0.0   \n3      0.493151  0.150486       0.400000      0.000000           0.0   \n4      0.150685  0.220635       0.800000      0.000000           0.0   \n...         ...       ...            ...           ...           ...   \n48837  0.301370  0.137428       0.800000      0.000000           0.0   \n48838  0.643836  0.209130       0.533333      0.000000           0.0   \n48839  0.287671  0.245379       0.800000      0.000000           0.0   \n48840  0.369863  0.048444       0.800000      0.054551           0.0   \n48841  0.246575  0.114919       0.800000      0.000000           0.0   \n\n       hours-per-week  workclass_ ?  workclass_ Federal-gov   \n0            0.397959           0.0                     0.0  \\\n1            0.122449           0.0                     0.0   \n2            0.397959           0.0                     0.0   \n3            0.397959           0.0                     0.0   \n4            0.397959           0.0                     0.0   \n...               ...           ...                     ...   \n48837        0.357143           0.0                     0.0   \n48838        0.397959           1.0                     0.0   \n48839        0.500000           0.0                     0.0   \n48840        0.397959           0.0                     0.0   \n48841        0.602041           0.0                     0.0   \n\n       workclass_ Local-gov  workclass_ Never-worked  ...   \n0                       0.0                      0.0  ...  \\\n1                       0.0                      0.0  ...   \n2                       0.0                      0.0  ...   \n3                       0.0                      0.0  ...   \n4                       0.0                      0.0  ...   \n...                     ...                      ...  ...   \n48837                   0.0                      0.0  ...   \n48838                   0.0                      0.0  ...   \n48839                   0.0                      0.0  ...   \n48840                   0.0                      0.0  ...   \n48841                   0.0                      0.0  ...   \n\n       native-country_ Portugal  native-country_ Puerto-Rico   \n0                           0.0                          0.0  \\\n1                           0.0                          0.0   \n2                           0.0                          0.0   \n3                           0.0                          0.0   \n4                           0.0                          0.0   \n...                         ...                          ...   \n48837                       0.0                          0.0   \n48838                       0.0                          0.0   \n48839                       0.0                          0.0   \n48840                       0.0                          0.0   \n48841                       0.0                          0.0   \n\n       native-country_ Scotland  native-country_ South   \n0                           0.0                    0.0  \\\n1                           0.0                    0.0   \n2                           0.0                    0.0   \n3                           0.0                    0.0   \n4                           0.0                    0.0   \n...                         ...                    ...   \n48837                       0.0                    0.0   \n48838                       0.0                    0.0   \n48839                       0.0                    0.0   \n48840                       0.0                    0.0   \n48841                       0.0                    0.0   \n\n       native-country_ Taiwan  native-country_ Thailand   \n0                         0.0                       0.0  \\\n1                         0.0                       0.0   \n2                         0.0                       0.0   \n3                         0.0                       0.0   \n4                         0.0                       0.0   \n...                       ...                       ...   \n48837                     0.0                       0.0   \n48838                     0.0                       0.0   \n48839                     0.0                       0.0   \n48840                     0.0                       0.0   \n48841                     0.0                       0.0   \n\n       native-country_ Trinadad&Tobago  native-country_ United-States   \n0                                  0.0                            1.0  \\\n1                                  0.0                            1.0   \n2                                  0.0                            1.0   \n3                                  0.0                            1.0   \n4                                  0.0                            0.0   \n...                                ...                            ...   \n48837                              0.0                            1.0   \n48838                              0.0                            1.0   \n48839                              0.0                            1.0   \n48840                              0.0                            1.0   \n48841                              0.0                            1.0   \n\n       native-country_ Vietnam  native-country_ Yugoslavia  \n0                          0.0                         0.0  \n1                          0.0                         0.0  \n2                          0.0                         0.0  \n3                          0.0                         0.0  \n4                          0.0                         0.0  \n...                        ...                         ...  \n48837                      0.0                         0.0  \n48838                      0.0                         0.0  \n48839                      0.0                         0.0  \n48840                      0.0                         0.0  \n48841                      0.0                         0.0  \n\n[48842 rows x 108 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>fnlwgt</th>\n      <th>education-num</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>workclass_ ?</th>\n      <th>workclass_ Federal-gov</th>\n      <th>workclass_ Local-gov</th>\n      <th>workclass_ Never-worked</th>\n      <th>...</th>\n      <th>native-country_ Portugal</th>\n      <th>native-country_ Puerto-Rico</th>\n      <th>native-country_ Scotland</th>\n      <th>native-country_ South</th>\n      <th>native-country_ Taiwan</th>\n      <th>native-country_ Thailand</th>\n      <th>native-country_ Trinadad&amp;Tobago</th>\n      <th>native-country_ United-States</th>\n      <th>native-country_ Vietnam</th>\n      <th>native-country_ Yugoslavia</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>0.044131</td>\n      <td>0.800000</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>0.048052</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>0.137581</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>0.150486</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>0.220635</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>0.137428</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0.209130</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>0.245379</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>0.048444</td>\n      <td>0.800000</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>0.114919</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 108 columns</p>\n</div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_feature(df, feature_to_encode):\n",
    "    dummies = pd.get_dummies(df[[feature_to_encode]], dtype=float)\n",
    "    result_df = pd.concat([df, dummies], axis=1)\n",
    "    result_df.drop(columns=feature_to_encode, inplace=True)\n",
    "    return result_df\n",
    "\n",
    "df_one_hot = og_df.copy()\n",
    "df_one_hot.drop(columns=\"class\", inplace=True)\n",
    "df_one_hot[cont_cols] = MinMaxScaler().fit_transform(df_one_hot[cont_cols])\n",
    "for col in cat_cols:\n",
    "    df_one_hot = encode_feature(df_one_hot, col)\n",
    "df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7165758977928832"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(df_one_hot)\n",
    "kmeans_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "kmeans_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.885593Z",
     "end_time": "2023-07-02T16:24:29.995676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1325918200579342"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "kmeans_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:29.990676Z",
     "end_time": "2023-07-02T16:24:30.060455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "# no_target_df = og_df.drop(columns=\"class\")\n",
    "# distance_matrix = gower_matrix_duped(no_target_df)\n",
    "# distance_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.041719Z",
     "end_time": "2023-07-02T16:24:30.061035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "# gower_agglo = AgglomerativeClustering(n_clusters=2, metric=\"precomputed\", linkage=\"single\").fit_predict(distance_matrix)\n",
    "# gower_agglo_acc = cluster_accuracy(gower_agglo, og_df[\"class\"].to_numpy())\n",
    "# gower_agglo_acc\n",
    "# linkage=average: 0.7602882764833545\n",
    "# linkage=single: 0.760697760124483"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.045029Z",
     "end_time": "2023-07-02T16:24:30.061035Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "# gower_agglo_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), gower_agglo)\n",
    "# gower_agglo_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.054454Z",
     "end_time": "2023-07-02T16:24:30.062547Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.063543Z",
     "end_time": "2023-07-02T16:24:30.077326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(9, 5), (16, 8), (7, 4), (15, 8), (6, 3), (5, 3), (2, 2), (42, 21)]"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(df[col].nunique(), min(50, max(2, (df[col].nunique()+1) // 2))) for col in df[cat_cols]]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.078327Z",
     "end_time": "2023-07-02T16:24:30.121037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "48842"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CensusIncomeDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "    \n",
    "dataset = CensusIncomeDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T16:24:30.091034Z",
     "end_time": "2023-07-02T16:25:45.026968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 1.020566\n",
      "epoch: 2/100, loss = 0.739358\n",
      "epoch: 3/100, loss = 0.715952\n",
      "epoch: 4/100, loss = 0.687372\n",
      "epoch: 5/100, loss = 0.658177\n",
      "epoch: 6/100, loss = 0.632254\n",
      "epoch: 7/100, loss = 0.612679\n",
      "epoch: 8/100, loss = 0.598682\n",
      "epoch: 9/100, loss = 0.586109\n",
      "epoch: 10/100, loss = 0.571799\n",
      "epoch: 11/100, loss = 0.564632\n",
      "epoch: 12/100, loss = 0.560129\n",
      "epoch: 13/100, loss = 0.557023\n",
      "epoch: 14/100, loss = 0.551485\n",
      "epoch: 15/100, loss = 0.547983\n",
      "epoch: 16/100, loss = 0.544542\n",
      "epoch: 17/100, loss = 0.541890\n",
      "epoch: 18/100, loss = 0.540489\n",
      "epoch: 19/100, loss = 0.538366\n",
      "epoch: 20/100, loss = 0.536244\n",
      "epoch: 21/100, loss = 0.533262\n",
      "epoch: 22/100, loss = 0.531448\n",
      "epoch: 23/100, loss = 0.529403\n",
      "epoch: 24/100, loss = 0.527366\n",
      "epoch: 25/100, loss = 0.524585\n",
      "epoch: 26/100, loss = 0.522880\n",
      "epoch: 27/100, loss = 0.520902\n",
      "epoch: 28/100, loss = 0.519908\n",
      "epoch: 29/100, loss = 0.519305\n",
      "epoch: 30/100, loss = 0.518359\n",
      "epoch: 31/100, loss = 0.517253\n",
      "epoch: 32/100, loss = 0.514751\n",
      "epoch: 33/100, loss = 0.512602\n",
      "epoch: 34/100, loss = 0.510734\n",
      "epoch: 35/100, loss = 0.509261\n",
      "epoch: 36/100, loss = 0.508247\n",
      "epoch: 37/100, loss = 0.507707\n",
      "epoch: 38/100, loss = 0.506541\n",
      "epoch: 39/100, loss = 0.506324\n",
      "epoch: 40/100, loss = 0.505028\n",
      "epoch: 41/100, loss = 0.504848\n",
      "epoch: 42/100, loss = 0.504164\n",
      "epoch: 43/100, loss = 0.503699\n",
      "epoch: 44/100, loss = 0.503352\n",
      "epoch: 45/100, loss = 0.502835\n",
      "epoch: 46/100, loss = 0.502906\n",
      "epoch: 47/100, loss = 0.502917\n",
      "epoch: 48/100, loss = 0.503032\n",
      "epoch: 49/100, loss = 0.502938\n",
      "epoch: 50/100, loss = 0.503272\n",
      "epoch: 51/100, loss = 0.503698\n",
      "epoch: 52/100, loss = 0.504243\n",
      "epoch: 53/100, loss = 0.504946\n",
      "epoch: 54/100, loss = 0.505386\n",
      "epoch: 55/100, loss = 0.505428\n",
      "epoch: 56/100, loss = 0.505522\n",
      "epoch: 57/100, loss = 0.505969\n",
      "epoch: 58/100, loss = 0.505677\n",
      "epoch: 59/100, loss = 0.506262\n",
      "epoch: 60/100, loss = 0.506723\n",
      "epoch: 61/100, loss = 0.506971\n",
      "epoch: 62/100, loss = 0.507755\n",
      "epoch: 63/100, loss = 0.507556\n",
      "epoch: 64/100, loss = 0.508442\n",
      "epoch: 65/100, loss = 0.509762\n",
      "epoch: 66/100, loss = 0.509365\n",
      "epoch: 67/100, loss = 0.509909\n",
      "epoch: 68/100, loss = 0.511212\n",
      "epoch: 69/100, loss = 0.512811\n",
      "epoch: 70/100, loss = 0.514265\n",
      "epoch: 71/100, loss = 0.515011\n",
      "epoch: 72/100, loss = 0.516635\n",
      "epoch: 73/100, loss = 0.517653\n",
      "epoch: 74/100, loss = 0.519204\n",
      "epoch: 75/100, loss = 0.520611\n",
      "epoch: 76/100, loss = 0.520165\n",
      "epoch: 77/100, loss = 0.521496\n",
      "epoch: 78/100, loss = 0.522935\n",
      "epoch: 79/100, loss = 0.525042\n",
      "epoch: 80/100, loss = 0.527102\n",
      "epoch: 81/100, loss = 0.528833\n",
      "epoch: 82/100, loss = 0.531035\n",
      "epoch: 83/100, loss = 0.532303\n",
      "epoch: 84/100, loss = 0.534544\n",
      "epoch: 85/100, loss = 0.537331\n",
      "epoch: 86/100, loss = 0.538883\n",
      "epoch: 87/100, loss = 0.540420\n",
      "epoch: 88/100, loss = 0.542700\n",
      "epoch: 89/100, loss = 0.546109\n",
      "epoch: 90/100, loss = 0.547257\n",
      "epoch: 91/100, loss = 0.548422\n",
      "epoch: 92/100, loss = 0.549988\n",
      "epoch: 93/100, loss = 0.551173\n",
      "epoch: 94/100, loss = 0.553690\n",
      "epoch: 95/100, loss = 0.555342\n",
      "epoch: 96/100, loss = 0.557617\n",
      "epoch: 97/100, loss = 0.560024\n",
      "epoch: 98/100, loss = 0.562587\n",
      "epoch: 99/100, loss = 0.564170\n",
      "epoch: 100/100, loss = 0.566251\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderOnlyCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, n_emb),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "model = AttentionModelDecoderOnlyCat()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, model.last_target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00373263, 0.23420216, 0.73741055, ..., 0.02174022, 0.        ,\n        0.39795918],\n       [0.70707089, 0.82034928, 0.05081815, ..., 0.        , 0.        ,\n        0.12244898],\n       [0.89275414, 0.40975994, 0.96112174, ..., 0.        , 0.        ,\n        0.39795918],\n       ...,\n       [0.919429  , 0.78858161, 0.33680543, ..., 0.        , 0.        ,\n        0.5       ],\n       [0.59791583, 0.49842462, 0.88033026, ..., 0.05455055, 0.        ,\n        0.39795918],\n       [0.74726337, 0.8134082 , 0.35902864, ..., 0.        , 0.        ,\n        0.60204082]])"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "cat_features = model.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, df[cont_cols].values), 1)\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:25:45.027969Z",
     "end_time": "2023-07-02T16:25:52.373239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6861717374390893"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "deep_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "deep_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:25:52.375219Z",
     "end_time": "2023-07-02T16:25:52.421353Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1509622343157827"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "deep_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:25:52.418353Z",
     "end_time": "2023-07-02T16:25:52.472297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 1.200897\n",
      "epoch: 2/100, loss = 0.905325\n",
      "epoch: 3/100, loss = 0.880776\n",
      "epoch: 4/100, loss = 0.847830\n",
      "epoch: 5/100, loss = 0.810408\n",
      "epoch: 6/100, loss = 0.764944\n",
      "epoch: 7/100, loss = 0.734826\n",
      "epoch: 8/100, loss = 0.722392\n",
      "epoch: 9/100, loss = 0.713046\n",
      "epoch: 10/100, loss = 0.705877\n",
      "epoch: 11/100, loss = 0.698123\n",
      "epoch: 12/100, loss = 0.691331\n",
      "epoch: 13/100, loss = 0.684895\n",
      "epoch: 14/100, loss = 0.680731\n",
      "epoch: 15/100, loss = 0.677711\n",
      "epoch: 16/100, loss = 0.674011\n",
      "epoch: 17/100, loss = 0.672450\n",
      "epoch: 18/100, loss = 0.671405\n",
      "epoch: 19/100, loss = 0.669366\n",
      "epoch: 20/100, loss = 0.667443\n",
      "epoch: 21/100, loss = 0.666632\n",
      "epoch: 22/100, loss = 0.666788\n",
      "epoch: 23/100, loss = 0.666079\n",
      "epoch: 24/100, loss = 0.663738\n",
      "epoch: 25/100, loss = 0.660467\n",
      "epoch: 26/100, loss = 0.658653\n",
      "epoch: 27/100, loss = 0.657672\n",
      "epoch: 28/100, loss = 0.656756\n",
      "epoch: 29/100, loss = 0.656332\n",
      "epoch: 30/100, loss = 0.655907\n",
      "epoch: 31/100, loss = 0.654801\n",
      "epoch: 32/100, loss = 0.654105\n",
      "epoch: 33/100, loss = 0.653823\n",
      "epoch: 34/100, loss = 0.653388\n",
      "epoch: 35/100, loss = 0.652795\n",
      "epoch: 36/100, loss = 0.650447\n",
      "epoch: 37/100, loss = 0.649248\n",
      "epoch: 38/100, loss = 0.648859\n",
      "epoch: 39/100, loss = 0.648049\n",
      "epoch: 40/100, loss = 0.647510\n",
      "epoch: 41/100, loss = 0.647324\n",
      "epoch: 42/100, loss = 0.647142\n",
      "epoch: 43/100, loss = 0.646058\n",
      "epoch: 44/100, loss = 0.646430\n",
      "epoch: 45/100, loss = 0.645077\n",
      "epoch: 46/100, loss = 0.645446\n",
      "epoch: 47/100, loss = 0.646835\n",
      "epoch: 48/100, loss = 0.647316\n",
      "epoch: 49/100, loss = 0.647815\n",
      "epoch: 50/100, loss = 0.647356\n",
      "epoch: 51/100, loss = 0.647828\n",
      "epoch: 52/100, loss = 0.649002\n",
      "epoch: 53/100, loss = 0.649935\n",
      "epoch: 54/100, loss = 0.650314\n",
      "epoch: 55/100, loss = 0.650778\n",
      "epoch: 56/100, loss = 0.653259\n",
      "epoch: 57/100, loss = 0.654253\n",
      "epoch: 58/100, loss = 0.655582\n",
      "epoch: 59/100, loss = 0.655781\n",
      "epoch: 60/100, loss = 0.656113\n",
      "epoch: 61/100, loss = 0.656460\n",
      "epoch: 62/100, loss = 0.656600\n",
      "epoch: 63/100, loss = 0.656839\n",
      "epoch: 64/100, loss = 0.656647\n",
      "epoch: 65/100, loss = 0.657784\n",
      "epoch: 66/100, loss = 0.658182\n",
      "epoch: 67/100, loss = 0.657950\n",
      "epoch: 68/100, loss = 0.658223\n",
      "epoch: 69/100, loss = 0.659121\n",
      "epoch: 70/100, loss = 0.660161\n",
      "epoch: 71/100, loss = 0.660963\n",
      "epoch: 72/100, loss = 0.663146\n",
      "epoch: 73/100, loss = 0.663686\n",
      "epoch: 74/100, loss = 0.665341\n",
      "epoch: 75/100, loss = 0.666050\n",
      "epoch: 76/100, loss = 0.667275\n",
      "epoch: 77/100, loss = 0.667277\n",
      "epoch: 78/100, loss = 0.669183\n",
      "epoch: 79/100, loss = 0.669810\n",
      "epoch: 80/100, loss = 0.670425\n",
      "epoch: 81/100, loss = 0.671584\n",
      "epoch: 82/100, loss = 0.672832\n",
      "epoch: 83/100, loss = 0.673919\n",
      "epoch: 84/100, loss = 0.674919\n",
      "epoch: 85/100, loss = 0.675560\n",
      "epoch: 86/100, loss = 0.676136\n",
      "epoch: 87/100, loss = 0.677848\n",
      "epoch: 88/100, loss = 0.678215\n",
      "epoch: 89/100, loss = 0.678572\n",
      "epoch: 90/100, loss = 0.678724\n",
      "epoch: 91/100, loss = 0.681055\n",
      "epoch: 92/100, loss = 0.681991\n",
      "epoch: 93/100, loss = 0.683239\n",
      "epoch: 94/100, loss = 0.683532\n",
      "epoch: 95/100, loss = 0.684262\n",
      "epoch: 96/100, loss = 0.687663\n",
      "epoch: 97/100, loss = 0.691285\n",
      "epoch: 98/100, loss = 0.693048\n",
      "epoch: 99/100, loss = 0.695365\n",
      "epoch: 100/100, loss = 0.697363\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderAllCols(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, in_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "all_cols_model = AttentionModelDecoderAllCols()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(all_cols_model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    all_cols_model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = all_cols_model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, torch.cat((all_cols_model.last_target, x_cont), 1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:25:52.470299Z",
     "end_time": "2023-07-02T16:27:07.578694Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.7042094 , 0.85504484, 0.0411601 , ..., 0.10365421, 0.9476202 ,\n        0.11772903],\n       [0.95396346, 0.87433374, 0.05289065, ..., 0.23917642, 0.9855456 ,\n        0.10095514],\n       [0.38521087, 0.7842759 , 0.6272747 , ..., 0.36176047, 0.21811807,\n        0.554441  ],\n       ...,\n       [0.6094985 , 0.9468627 , 0.02152351, ..., 0.6661652 , 0.4854455 ,\n        0.05027826],\n       [0.2231823 , 0.9703561 , 0.06150167, ..., 0.913161  , 0.5171215 ,\n        0.04834109],\n       [0.94233865, 0.9083051 , 0.05359063, ..., 0.4134207 , 0.96957076,\n        0.08910792]], dtype=float32)"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "decoder_all_cols_features = all_cols_model.encode(cat, cont).detach().numpy()\n",
    "decoder_all_cols_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:27:07.578694Z",
     "end_time": "2023-07-02T16:27:14.958674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5973137873141968"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(decoder_all_cols_features)\n",
    "all_cols_acc = cluster_accuracy(all_cols_kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "all_cols_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:27:14.960675Z",
     "end_time": "2023-07-02T16:27:15.007267Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "0.011069566474679665"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), all_cols_kmeans.labels_)\n",
    "all_cols_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:27:15.005267Z",
     "end_time": "2023-07-02T16:27:15.078772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Accuracy       NMI\nKMeans                                              0.716576  0.132592\nGower + Agglomerative                               0.760698  0.000023\nDeep Attention KMeans, only Cat Cols reconstruc...  0.686172  0.150962\nDeep Attention KMeans, all Cols reconstructed i...  0.597314  0.011070",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>NMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>KMeans</th>\n      <td>0.716576</td>\n      <td>0.132592</td>\n    </tr>\n    <tr>\n      <th>Gower + Agglomerative</th>\n      <td>0.760698</td>\n      <td>0.000023</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, only Cat Cols reconstructed in Decoder</th>\n      <td>0.686172</td>\n      <td>0.150962</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, all Cols reconstructed in Decoder</th>\n      <td>0.597314</td>\n      <td>0.011070</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[kmeans_acc, kmeans_nmi], [0.760697760124483, 0.000023], [deep_acc, deep_nmi], [all_cols_acc, all_cols_nmi]], index=[\"KMeans\", \"Gower + Agglomerative\", \"Deep Attention KMeans, only Cat Cols reconstructed in Decoder\", \"Deep Attention KMeans, all Cols reconstructed in Decoder\"], columns=[\"Accuracy\", \"NMI\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:27:15.056531Z",
     "end_time": "2023-07-02T16:27:15.078772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T16:27:15.064072Z",
     "end_time": "2023-07-02T16:27:15.078772Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
