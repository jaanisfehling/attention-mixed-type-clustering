{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.508707Z",
     "end_time": "2023-07-02T15:55:07.522874Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from gower_duped import gower_matrix as gower_matrix_duped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.511706Z",
     "end_time": "2023-07-02T15:55:07.523874Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_accuracy(y_pred, y_true):\n",
    "    # We need to map the labels to our cluster labels\n",
    "    # This is a linear assignment problem on a bipartite graph\n",
    "    k = max(len(np.unique(y_pred)), len(np.unique(y_pred)))\n",
    "    cost_matrix = np.zeros((k, k))\n",
    "    for i in range(y_pred.size):\n",
    "        cost_matrix[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix.max() - cost_matrix)\n",
    "    return cost_matrix[row_ind, col_ind].sum() / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.519876Z",
     "end_time": "2023-07-02T15:55:07.597082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       age          workclass  fnlwgt   education  education-num   \n0       39          State-gov   77516   Bachelors             13  \\\n1       50   Self-emp-not-inc   83311   Bachelors             13   \n2       38            Private  215646     HS-grad              9   \n3       53            Private  234721        11th              7   \n4       28            Private  338409   Bachelors             13   \n...    ...                ...     ...         ...            ...   \n48837   39            Private  215419   Bachelors             13   \n48838   64                  ?  321403     HS-grad              9   \n48839   38            Private  374983   Bachelors             13   \n48840   44            Private   83891   Bachelors             13   \n48841   35       Self-emp-inc  182148   Bachelors             13   \n\n            marital-status          occupation     relationship   \n0            Never-married        Adm-clerical    Not-in-family  \\\n1       Married-civ-spouse     Exec-managerial          Husband   \n2                 Divorced   Handlers-cleaners    Not-in-family   \n3       Married-civ-spouse   Handlers-cleaners          Husband   \n4       Married-civ-spouse      Prof-specialty             Wife   \n...                    ...                 ...              ...   \n48837             Divorced      Prof-specialty    Not-in-family   \n48838              Widowed                   ?   Other-relative   \n48839   Married-civ-spouse      Prof-specialty          Husband   \n48840             Divorced        Adm-clerical        Own-child   \n48841   Married-civ-spouse     Exec-managerial          Husband   \n\n                      race      sex  capital-gain  capital-loss   \n0                    White     Male          2174             0  \\\n1                    White     Male             0             0   \n2                    White     Male             0             0   \n3                    Black     Male             0             0   \n4                    Black   Female             0             0   \n...                    ...      ...           ...           ...   \n48837                White   Female             0             0   \n48838                Black     Male             0             0   \n48839                White     Male             0             0   \n48840   Asian-Pac-Islander     Male          5455             0   \n48841                White     Male             0             0   \n\n       hours-per-week  native-country    class  \n0                  40   United-States    <=50K  \n1                  13   United-States    <=50K  \n2                  40   United-States    <=50K  \n3                  40   United-States    <=50K  \n4                  40            Cuba    <=50K  \n...               ...             ...      ...  \n48837              36   United-States   <=50K.  \n48838              40   United-States   <=50K.  \n48839              50   United-States   <=50K.  \n48840              40   United-States   <=50K.  \n48841              60   United-States    >50K.  \n\n[48842 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>39</td>\n      <td>Private</td>\n      <td>215419</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Prof-specialty</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>64</td>\n      <td>?</td>\n      <td>321403</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Widowed</td>\n      <td>?</td>\n      <td>Other-relative</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>374983</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>50</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>44</td>\n      <td>Private</td>\n      <td>83891</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Divorced</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>Asian-Pac-Islander</td>\n      <td>Male</td>\n      <td>5455</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K.</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>35</td>\n      <td>Self-emp-inc</td>\n      <td>182148</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>United-States</td>\n      <td>&gt;50K.</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df = pd.read_csv(\"datasets/census_income.csv\")\n",
    "og_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7607182343065395"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df.loc[(og_df[\"class\"] == \" <=50K.\") | (og_df[\"class\"] == \" <=50K\"), \"class\"] = 0\n",
    "og_df.loc[(og_df[\"class\"] == \" >50K.\") | (og_df[\"class\"] == \" >50K\"), \"class\"] = 1\n",
    "# Probability of most common class\n",
    "og_df[\"class\"].value_counts().max()/og_df[\"class\"].count()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.599084Z",
     "end_time": "2023-07-02T15:55:07.615377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.616380Z",
     "end_time": "2023-07-02T15:55:07.637170Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "cont_cols = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.620834Z",
     "end_time": "2023-07-02T15:55:07.725828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age  workclass    fnlwgt  education  education-num   \n0      0.301370          7  0.044131          9       0.800000  \\\n1      0.452055          6  0.048052          9       0.800000   \n2      0.287671          4  0.137581         11       0.533333   \n3      0.493151          4  0.150486          1       0.400000   \n4      0.150685          4  0.220635          9       0.800000   \n...         ...        ...       ...        ...            ...   \n48837  0.301370          4  0.137428          9       0.800000   \n48838  0.643836          0  0.209130         11       0.533333   \n48839  0.287671          4  0.245379          9       0.800000   \n48840  0.369863          4  0.048444          9       0.800000   \n48841  0.246575          5  0.114919          9       0.800000   \n\n       marital-status  occupation  relationship  race  sex  capital-gain   \n0                   4           1             1     4    1      0.021740  \\\n1                   2           4             0     4    1      0.000000   \n2                   0           6             1     4    1      0.000000   \n3                   2           6             0     2    1      0.000000   \n4                   2          10             5     2    0      0.000000   \n...               ...         ...           ...   ...  ...           ...   \n48837               0          10             1     4    0      0.000000   \n48838               6           0             2     2    1      0.000000   \n48839               2          10             0     4    1      0.000000   \n48840               0           1             3     1    1      0.054551   \n48841               2           4             0     4    1      0.000000   \n\n       capital-loss  hours-per-week  native-country  \n0               0.0        0.397959              39  \n1               0.0        0.122449              39  \n2               0.0        0.397959              39  \n3               0.0        0.397959              39  \n4               0.0        0.397959               5  \n...             ...             ...             ...  \n48837           0.0        0.357143              39  \n48838           0.0        0.397959              39  \n48839           0.0        0.500000              39  \n48840           0.0        0.397959              39  \n48841           0.0        0.602041              39  \n\n[48842 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>7</td>\n      <td>0.044131</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>6</td>\n      <td>0.048052</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.137581</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>4</td>\n      <td>0.150486</td>\n      <td>1</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>4</td>\n      <td>0.220635</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>4</td>\n      <td>0.137428</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0</td>\n      <td>0.209130</td>\n      <td>11</td>\n      <td>0.533333</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>4</td>\n      <td>0.245379</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>10</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>4</td>\n      <td>0.048444</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>5</td>\n      <td>0.114919</td>\n      <td>9</td>\n      <td>0.800000</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = og_df.copy()\n",
    "df.drop(columns=\"class\", inplace=True)\n",
    "df[cat_cols] = df[cat_cols].apply(LabelEncoder().fit_transform)\n",
    "df[cont_cols] = MinMaxScaler().fit_transform(df[cont_cols])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.688813Z",
     "end_time": "2023-07-02T15:55:07.889741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            age    fnlwgt  education-num  capital-gain  capital-loss   \n0      0.301370  0.044131       0.800000      0.021740           0.0  \\\n1      0.452055  0.048052       0.800000      0.000000           0.0   \n2      0.287671  0.137581       0.533333      0.000000           0.0   \n3      0.493151  0.150486       0.400000      0.000000           0.0   \n4      0.150685  0.220635       0.800000      0.000000           0.0   \n...         ...       ...            ...           ...           ...   \n48837  0.301370  0.137428       0.800000      0.000000           0.0   \n48838  0.643836  0.209130       0.533333      0.000000           0.0   \n48839  0.287671  0.245379       0.800000      0.000000           0.0   \n48840  0.369863  0.048444       0.800000      0.054551           0.0   \n48841  0.246575  0.114919       0.800000      0.000000           0.0   \n\n       hours-per-week  workclass_ ?  workclass_ Federal-gov   \n0            0.397959           0.0                     0.0  \\\n1            0.122449           0.0                     0.0   \n2            0.397959           0.0                     0.0   \n3            0.397959           0.0                     0.0   \n4            0.397959           0.0                     0.0   \n...               ...           ...                     ...   \n48837        0.357143           0.0                     0.0   \n48838        0.397959           1.0                     0.0   \n48839        0.500000           0.0                     0.0   \n48840        0.397959           0.0                     0.0   \n48841        0.602041           0.0                     0.0   \n\n       workclass_ Local-gov  workclass_ Never-worked  ...   \n0                       0.0                      0.0  ...  \\\n1                       0.0                      0.0  ...   \n2                       0.0                      0.0  ...   \n3                       0.0                      0.0  ...   \n4                       0.0                      0.0  ...   \n...                     ...                      ...  ...   \n48837                   0.0                      0.0  ...   \n48838                   0.0                      0.0  ...   \n48839                   0.0                      0.0  ...   \n48840                   0.0                      0.0  ...   \n48841                   0.0                      0.0  ...   \n\n       native-country_ Portugal  native-country_ Puerto-Rico   \n0                           0.0                          0.0  \\\n1                           0.0                          0.0   \n2                           0.0                          0.0   \n3                           0.0                          0.0   \n4                           0.0                          0.0   \n...                         ...                          ...   \n48837                       0.0                          0.0   \n48838                       0.0                          0.0   \n48839                       0.0                          0.0   \n48840                       0.0                          0.0   \n48841                       0.0                          0.0   \n\n       native-country_ Scotland  native-country_ South   \n0                           0.0                    0.0  \\\n1                           0.0                    0.0   \n2                           0.0                    0.0   \n3                           0.0                    0.0   \n4                           0.0                    0.0   \n...                         ...                    ...   \n48837                       0.0                    0.0   \n48838                       0.0                    0.0   \n48839                       0.0                    0.0   \n48840                       0.0                    0.0   \n48841                       0.0                    0.0   \n\n       native-country_ Taiwan  native-country_ Thailand   \n0                         0.0                       0.0  \\\n1                         0.0                       0.0   \n2                         0.0                       0.0   \n3                         0.0                       0.0   \n4                         0.0                       0.0   \n...                       ...                       ...   \n48837                     0.0                       0.0   \n48838                     0.0                       0.0   \n48839                     0.0                       0.0   \n48840                     0.0                       0.0   \n48841                     0.0                       0.0   \n\n       native-country_ Trinadad&Tobago  native-country_ United-States   \n0                                  0.0                            1.0  \\\n1                                  0.0                            1.0   \n2                                  0.0                            1.0   \n3                                  0.0                            1.0   \n4                                  0.0                            0.0   \n...                                ...                            ...   \n48837                              0.0                            1.0   \n48838                              0.0                            1.0   \n48839                              0.0                            1.0   \n48840                              0.0                            1.0   \n48841                              0.0                            1.0   \n\n       native-country_ Vietnam  native-country_ Yugoslavia  \n0                          0.0                         0.0  \n1                          0.0                         0.0  \n2                          0.0                         0.0  \n3                          0.0                         0.0  \n4                          0.0                         0.0  \n...                        ...                         ...  \n48837                      0.0                         0.0  \n48838                      0.0                         0.0  \n48839                      0.0                         0.0  \n48840                      0.0                         0.0  \n48841                      0.0                         0.0  \n\n[48842 rows x 108 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>fnlwgt</th>\n      <th>education-num</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>workclass_ ?</th>\n      <th>workclass_ Federal-gov</th>\n      <th>workclass_ Local-gov</th>\n      <th>workclass_ Never-worked</th>\n      <th>...</th>\n      <th>native-country_ Portugal</th>\n      <th>native-country_ Puerto-Rico</th>\n      <th>native-country_ Scotland</th>\n      <th>native-country_ South</th>\n      <th>native-country_ Taiwan</th>\n      <th>native-country_ Thailand</th>\n      <th>native-country_ Trinadad&amp;Tobago</th>\n      <th>native-country_ United-States</th>\n      <th>native-country_ Vietnam</th>\n      <th>native-country_ Yugoslavia</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.301370</td>\n      <td>0.044131</td>\n      <td>0.800000</td>\n      <td>0.021740</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.452055</td>\n      <td>0.048052</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.122449</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.287671</td>\n      <td>0.137581</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493151</td>\n      <td>0.150486</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.150685</td>\n      <td>0.220635</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48837</th>\n      <td>0.301370</td>\n      <td>0.137428</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.357143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48838</th>\n      <td>0.643836</td>\n      <td>0.209130</td>\n      <td>0.533333</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48839</th>\n      <td>0.287671</td>\n      <td>0.245379</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48840</th>\n      <td>0.369863</td>\n      <td>0.048444</td>\n      <td>0.800000</td>\n      <td>0.054551</td>\n      <td>0.0</td>\n      <td>0.397959</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48841</th>\n      <td>0.246575</td>\n      <td>0.114919</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.602041</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48842 rows × 108 columns</p>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_feature(df, feature_to_encode):\n",
    "    dummies = pd.get_dummies(df[[feature_to_encode]], dtype=float)\n",
    "    result_df = pd.concat([df, dummies], axis=1)\n",
    "    result_df.drop(columns=feature_to_encode, inplace=True)\n",
    "    return result_df\n",
    "\n",
    "df_one_hot = og_df.copy()\n",
    "df_one_hot.drop(columns=\"class\", inplace=True)\n",
    "df_one_hot[cont_cols] = MinMaxScaler().fit_transform(df_one_hot[cont_cols])\n",
    "for col in cat_cols:\n",
    "    df_one_hot = encode_feature(df_one_hot, col)\n",
    "df_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7165758977928832"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(df_one_hot)\n",
    "kmeans_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "kmeans_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.891743Z",
     "end_time": "2023-07-02T15:55:08.003543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1325918200579342"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "kmeans_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:07.995544Z",
     "end_time": "2023-07-02T15:55:08.077078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "# no_target_df = og_df.drop(columns=\"class\")\n",
    "# distance_matrix = gower_matrix_duped(no_target_df)\n",
    "# distance_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.047136Z",
     "end_time": "2023-07-02T15:55:08.077078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# gower_agglo = AgglomerativeClustering(n_clusters=2, metric=\"precomputed\", linkage=\"single\").fit_predict(distance_matrix)\n",
    "# gower_agglo_acc = cluster_accuracy(gower_agglo, og_df[\"class\"].to_numpy())\n",
    "# gower_agglo_acc\n",
    "# linkage=average: 0.7602882764833545\n",
    "# linkage=single: 0.760697760124483"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.049648Z",
     "end_time": "2023-07-02T15:55:08.078078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# gower_agglo_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), gower_agglo)\n",
    "# gower_agglo_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.060010Z",
     "end_time": "2023-07-02T15:55:08.078078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.069082Z",
     "end_time": "2023-07-02T15:55:08.084039Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(9, 5), (16, 8), (7, 4), (15, 8), (6, 3), (5, 3), (2, 2), (42, 21)]"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sizes = [(df[col].nunique(), min(50, max(2, (df[col].nunique()+1) // 2))) for col in df[cat_cols]]\n",
    "embedding_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.087040Z",
     "end_time": "2023-07-02T15:55:08.095276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "48842"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CensusIncomeDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "        self.cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.cont[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cat.shape[0]\n",
    "    \n",
    "dataset = CensusIncomeDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-02T15:55:08.101278Z",
     "end_time": "2023-07-02T15:56:23.750183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 1.011161\n",
      "epoch: 2/100, loss = 0.749975\n",
      "epoch: 3/100, loss = 0.723298\n",
      "epoch: 4/100, loss = 0.701258\n",
      "epoch: 5/100, loss = 0.683748\n",
      "epoch: 6/100, loss = 0.673417\n",
      "epoch: 7/100, loss = 0.659925\n",
      "epoch: 8/100, loss = 0.646812\n",
      "epoch: 9/100, loss = 0.636555\n",
      "epoch: 10/100, loss = 0.627363\n",
      "epoch: 11/100, loss = 0.622448\n",
      "epoch: 12/100, loss = 0.618037\n",
      "epoch: 13/100, loss = 0.614590\n",
      "epoch: 14/100, loss = 0.611696\n",
      "epoch: 15/100, loss = 0.608681\n",
      "epoch: 16/100, loss = 0.605609\n",
      "epoch: 17/100, loss = 0.602997\n",
      "epoch: 18/100, loss = 0.601463\n",
      "epoch: 19/100, loss = 0.599820\n",
      "epoch: 20/100, loss = 0.598456\n",
      "epoch: 21/100, loss = 0.597155\n",
      "epoch: 22/100, loss = 0.596325\n",
      "epoch: 23/100, loss = 0.595582\n",
      "epoch: 24/100, loss = 0.595352\n",
      "epoch: 25/100, loss = 0.595104\n",
      "epoch: 26/100, loss = 0.594951\n",
      "epoch: 27/100, loss = 0.595158\n",
      "epoch: 28/100, loss = 0.596212\n",
      "epoch: 29/100, loss = 0.597184\n",
      "epoch: 30/100, loss = 0.598257\n",
      "epoch: 31/100, loss = 0.600083\n",
      "epoch: 32/100, loss = 0.603421\n",
      "epoch: 33/100, loss = 0.606581\n",
      "epoch: 34/100, loss = 0.607742\n",
      "epoch: 35/100, loss = 0.608278\n",
      "epoch: 36/100, loss = 0.608779\n",
      "epoch: 37/100, loss = 0.609407\n",
      "epoch: 38/100, loss = 0.610045\n",
      "epoch: 39/100, loss = 0.610937\n",
      "epoch: 40/100, loss = 0.611468\n",
      "epoch: 41/100, loss = 0.612696\n",
      "epoch: 42/100, loss = 0.614082\n",
      "epoch: 43/100, loss = 0.614655\n",
      "epoch: 44/100, loss = 0.615490\n",
      "epoch: 45/100, loss = 0.616553\n",
      "epoch: 46/100, loss = 0.618067\n",
      "epoch: 47/100, loss = 0.618823\n",
      "epoch: 48/100, loss = 0.620166\n",
      "epoch: 49/100, loss = 0.621712\n",
      "epoch: 50/100, loss = 0.623007\n",
      "epoch: 51/100, loss = 0.624212\n",
      "epoch: 52/100, loss = 0.625848\n",
      "epoch: 53/100, loss = 0.627081\n",
      "epoch: 54/100, loss = 0.628364\n",
      "epoch: 55/100, loss = 0.629054\n",
      "epoch: 56/100, loss = 0.630473\n",
      "epoch: 57/100, loss = 0.632155\n",
      "epoch: 58/100, loss = 0.634180\n",
      "epoch: 59/100, loss = 0.635590\n",
      "epoch: 60/100, loss = 0.637267\n",
      "epoch: 61/100, loss = 0.638242\n",
      "epoch: 62/100, loss = 0.639308\n",
      "epoch: 63/100, loss = 0.640283\n",
      "epoch: 64/100, loss = 0.641673\n",
      "epoch: 65/100, loss = 0.643036\n",
      "epoch: 66/100, loss = 0.644714\n",
      "epoch: 67/100, loss = 0.647021\n",
      "epoch: 68/100, loss = 0.648673\n",
      "epoch: 69/100, loss = 0.649918\n",
      "epoch: 70/100, loss = 0.651485\n",
      "epoch: 71/100, loss = 0.653644\n",
      "epoch: 72/100, loss = 0.655148\n",
      "epoch: 73/100, loss = 0.656791\n",
      "epoch: 74/100, loss = 0.658066\n",
      "epoch: 75/100, loss = 0.659769\n",
      "epoch: 76/100, loss = 0.661764\n",
      "epoch: 77/100, loss = 0.663371\n",
      "epoch: 78/100, loss = 0.665320\n",
      "epoch: 79/100, loss = 0.667067\n",
      "epoch: 80/100, loss = 0.669402\n",
      "epoch: 81/100, loss = 0.671618\n",
      "epoch: 82/100, loss = 0.673694\n",
      "epoch: 83/100, loss = 0.676244\n",
      "epoch: 84/100, loss = 0.679063\n",
      "epoch: 85/100, loss = 0.679118\n",
      "epoch: 86/100, loss = 0.677587\n",
      "epoch: 87/100, loss = 0.677384\n",
      "epoch: 88/100, loss = 0.678693\n",
      "epoch: 89/100, loss = 0.678602\n",
      "epoch: 90/100, loss = 0.679331\n",
      "epoch: 91/100, loss = 0.681598\n",
      "epoch: 92/100, loss = 0.683964\n",
      "epoch: 93/100, loss = 0.686173\n",
      "epoch: 94/100, loss = 0.688338\n",
      "epoch: 95/100, loss = 0.691240\n",
      "epoch: 96/100, loss = 0.693849\n",
      "epoch: 97/100, loss = 0.696160\n",
      "epoch: 98/100, loss = 0.697598\n",
      "epoch: 99/100, loss = 0.699901\n",
      "epoch: 100/100, loss = 0.701973\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderOnlyCat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, n_emb),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "model = AttentionModelDecoderOnlyCat()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, model.last_target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.27100515, -1.65021658,  1.78275776, ...,  0.02174022,\n         0.        ,  0.39795918],\n       [-3.20273423, -1.71422434, -0.46493387, ...,  0.        ,\n         0.        ,  0.12244898],\n       [ 0.45016956, -1.44210911, -3.24438381, ...,  0.        ,\n         0.        ,  0.39795918],\n       ...,\n       [-1.98420954, -1.61188459,  1.4049902 , ...,  0.        ,\n         0.        ,  0.5       ],\n       [-2.11292887, -1.72118902,  0.78204441, ...,  0.05455055,\n         0.        ,  0.39795918],\n       [-2.08278608, -1.822721  ,  1.02361584, ...,  0.        ,\n         0.        ,  0.60204082]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "cat_features = model.encode(cat, cont).detach().numpy()\n",
    "features = np.concatenate((cat_features, df[cont_cols].values), 1)\n",
    "features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:56:23.751182Z",
     "end_time": "2023-07-02T15:56:31.175872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5062650997092666"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(features)\n",
    "deep_acc = cluster_accuracy(kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "deep_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:56:31.177873Z",
     "end_time": "2023-07-02T15:56:31.231410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "0.021787362747908486"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), kmeans.labels_)\n",
    "deep_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:56:31.226079Z",
     "end_time": "2023-07-02T15:56:31.278868Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/100, loss = 0.802192\n",
      "epoch: 2/100, loss = 0.587430\n",
      "epoch: 3/100, loss = 0.563110\n",
      "epoch: 4/100, loss = 0.540228\n",
      "epoch: 5/100, loss = 0.524651\n",
      "epoch: 6/100, loss = 0.511783\n",
      "epoch: 7/100, loss = 0.503997\n",
      "epoch: 8/100, loss = 0.497053\n",
      "epoch: 9/100, loss = 0.490834\n",
      "epoch: 10/100, loss = 0.484748\n",
      "epoch: 11/100, loss = 0.480123\n",
      "epoch: 12/100, loss = 0.476143\n",
      "epoch: 13/100, loss = 0.473645\n",
      "epoch: 14/100, loss = 0.471910\n",
      "epoch: 15/100, loss = 0.470136\n",
      "epoch: 16/100, loss = 0.468560\n",
      "epoch: 17/100, loss = 0.467343\n",
      "epoch: 18/100, loss = 0.466572\n",
      "epoch: 19/100, loss = 0.464882\n",
      "epoch: 20/100, loss = 0.463597\n",
      "epoch: 21/100, loss = 0.462804\n",
      "epoch: 22/100, loss = 0.462486\n",
      "epoch: 23/100, loss = 0.461753\n",
      "epoch: 24/100, loss = 0.461230\n",
      "epoch: 25/100, loss = 0.460672\n",
      "epoch: 26/100, loss = 0.460670\n",
      "epoch: 27/100, loss = 0.460256\n",
      "epoch: 28/100, loss = 0.459365\n",
      "epoch: 29/100, loss = 0.459115\n",
      "epoch: 30/100, loss = 0.458987\n",
      "epoch: 31/100, loss = 0.458754\n",
      "epoch: 32/100, loss = 0.458768\n",
      "epoch: 33/100, loss = 0.458740\n",
      "epoch: 34/100, loss = 0.458219\n",
      "epoch: 35/100, loss = 0.457792\n",
      "epoch: 36/100, loss = 0.457727\n",
      "epoch: 37/100, loss = 0.457418\n",
      "epoch: 38/100, loss = 0.456685\n",
      "epoch: 39/100, loss = 0.456379\n",
      "epoch: 40/100, loss = 0.456018\n",
      "epoch: 41/100, loss = 0.455416\n",
      "epoch: 42/100, loss = 0.455182\n",
      "epoch: 43/100, loss = 0.454838\n",
      "epoch: 44/100, loss = 0.454578\n",
      "epoch: 45/100, loss = 0.453832\n",
      "epoch: 46/100, loss = 0.453058\n",
      "epoch: 47/100, loss = 0.452271\n",
      "epoch: 48/100, loss = 0.451802\n",
      "epoch: 49/100, loss = 0.451760\n",
      "epoch: 50/100, loss = 0.451478\n",
      "epoch: 51/100, loss = 0.451474\n",
      "epoch: 52/100, loss = 0.451355\n",
      "epoch: 53/100, loss = 0.451201\n",
      "epoch: 54/100, loss = 0.451316\n",
      "epoch: 55/100, loss = 0.451367\n",
      "epoch: 56/100, loss = 0.451497\n",
      "epoch: 57/100, loss = 0.451689\n",
      "epoch: 58/100, loss = 0.452034\n",
      "epoch: 59/100, loss = 0.452538\n",
      "epoch: 60/100, loss = 0.453022\n",
      "epoch: 61/100, loss = 0.453304\n",
      "epoch: 62/100, loss = 0.453137\n",
      "epoch: 63/100, loss = 0.453471\n",
      "epoch: 64/100, loss = 0.453852\n",
      "epoch: 65/100, loss = 0.454847\n",
      "epoch: 66/100, loss = 0.455624\n",
      "epoch: 67/100, loss = 0.456587\n",
      "epoch: 68/100, loss = 0.458042\n",
      "epoch: 69/100, loss = 0.458654\n",
      "epoch: 70/100, loss = 0.459096\n",
      "epoch: 71/100, loss = 0.460528\n",
      "epoch: 72/100, loss = 0.461184\n",
      "epoch: 73/100, loss = 0.462510\n",
      "epoch: 74/100, loss = 0.463898\n",
      "epoch: 75/100, loss = 0.465491\n",
      "epoch: 76/100, loss = 0.466885\n",
      "epoch: 77/100, loss = 0.468130\n",
      "epoch: 78/100, loss = 0.468975\n",
      "epoch: 79/100, loss = 0.470274\n",
      "epoch: 80/100, loss = 0.471779\n",
      "epoch: 81/100, loss = 0.473565\n",
      "epoch: 82/100, loss = 0.475760\n",
      "epoch: 83/100, loss = 0.478396\n",
      "epoch: 84/100, loss = 0.481709\n",
      "epoch: 85/100, loss = 0.484388\n",
      "epoch: 86/100, loss = 0.485983\n",
      "epoch: 87/100, loss = 0.487422\n",
      "epoch: 88/100, loss = 0.489592\n",
      "epoch: 89/100, loss = 0.491534\n",
      "epoch: 90/100, loss = 0.494655\n",
      "epoch: 91/100, loss = 0.496280\n",
      "epoch: 92/100, loss = 0.498099\n",
      "epoch: 93/100, loss = 0.498991\n",
      "epoch: 94/100, loss = 0.500530\n",
      "epoch: 95/100, loss = 0.502489\n",
      "epoch: 96/100, loss = 0.504332\n",
      "epoch: 97/100, loss = 0.505061\n",
      "epoch: 98/100, loss = 0.506847\n",
      "epoch: 99/100, loss = 0.509194\n",
      "epoch: 100/100, loss = 0.511628\n"
     ]
    }
   ],
   "source": [
    "class AttentionModelDecoderAllCols(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(num, dim) for num, dim in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        in_dim = n_emb + len(cont_cols)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(32),\n",
    "            torch.nn.Linear(32, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(16),\n",
    "            torch.nn.Linear(16, 8),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.BatchNorm1d(8),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, in_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x_cat, x_cont):\n",
    "        x_cat = x_cat.to(torch.long)\n",
    "        embedded = torch.cat([e(x_cat[:, i]) for i, e in enumerate(self.embeddings)], 1)\n",
    "        self.last_target = embedded.clone().detach()\n",
    "\n",
    "        qkv = torch.cat((embedded, x_cont), 1)\n",
    "        x = F.scaled_dot_product_attention(qkv, qkv, qkv)\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        encoded = self.encode(x_cat, x_cont)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "all_cols_model = AttentionModelDecoderAllCols()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(all_cols_model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    all_cols_model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for x_cat, x_cont in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = all_cols_model(x_cat, x_cont)\n",
    "        train_loss = criterion(outputs, torch.cat((all_cols_model.last_target, x_cont), 1))\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    loss = loss / len(dataloader)\n",
    "    print(\"epoch: {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:56:31.277867Z",
     "end_time": "2023-07-02T15:57:45.484995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.07509518, -0.32844543, -0.1906023 , ..., -0.56984806,\n         2.4054174 ,  1.4794273 ],\n       [-1.3279209 , -3.7389007 ,  2.0956469 , ...,  2.3532438 ,\n        -1.2456269 ,  0.368495  ],\n       [ 1.2748337 , -0.6890826 , -0.4127841 , ..., -2.7919455 ,\n         2.7442555 ,  0.33372307],\n       ...,\n       [-2.2810316 , -2.057921  , -0.6197653 , ...,  2.0288439 ,\n         1.324235  , -2.621729  ],\n       [-0.36414146, -0.73335123, -0.01869631, ..., -0.18686485,\n         2.5865703 , -0.45962954],\n       [-0.87611675, -0.8533535 ,  3.9578562 , ...,  1.2284737 ,\n        -0.6673651 , -3.7042866 ]], dtype=float32)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.tensor(df[cat_cols].values, dtype=torch.float)\n",
    "cont = torch.tensor(df[cont_cols].values, dtype=torch.float)\n",
    "decoder_all_cols_features = all_cols_model.encode(cat, cont).detach().numpy()\n",
    "decoder_all_cols_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:57:45.486995Z",
     "end_time": "2023-07-02T15:57:52.891614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6772859424266"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=0).fit(decoder_all_cols_features)\n",
    "all_cols_acc = cluster_accuracy(all_cols_kmeans.labels_, og_df[\"class\"].to_numpy())\n",
    "all_cols_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:57:52.891614Z",
     "end_time": "2023-07-02T15:57:52.934604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1469560146376703"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cols_nmi = normalized_mutual_info_score(og_df[\"class\"].to_numpy(), all_cols_kmeans.labels_)\n",
    "all_cols_nmi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:57:52.933609Z",
     "end_time": "2023-07-02T15:57:52.985815Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    Accuracy       NMI\nKMeans                                              0.716576  0.132592\nGower + Agglomerative                               0.760698  0.000023\nDeep Attention KMeans, only Cat Cols reconstruc...  0.506265  0.021787\nDeep Attention KMeans, all Cols reconstructed i...  0.677286  0.146956",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>NMI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>KMeans</th>\n      <td>0.716576</td>\n      <td>0.132592</td>\n    </tr>\n    <tr>\n      <th>Gower + Agglomerative</th>\n      <td>0.760698</td>\n      <td>0.000023</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, only Cat Cols reconstructed in Decoder</th>\n      <td>0.506265</td>\n      <td>0.021787</td>\n    </tr>\n    <tr>\n      <th>Deep Attention KMeans, all Cols reconstructed in Decoder</th>\n      <td>0.677286</td>\n      <td>0.146956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[kmeans_acc, kmeans_nmi], [0.760697760124483, 0.000023], [deep_acc, deep_nmi], [all_cols_acc, all_cols_nmi]], index=[\"KMeans\", \"Gower + Agglomerative\", \"Deep Attention KMeans, only Cat Cols reconstructed in Decoder\", \"Deep Attention KMeans, all Cols reconstructed in Decoder\"], columns=[\"Accuracy\", \"NMI\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:57:52.986815Z",
     "end_time": "2023-07-02T15:57:52.991816Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-02T15:57:52.991816Z",
     "end_time": "2023-07-02T15:57:52.994532Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
